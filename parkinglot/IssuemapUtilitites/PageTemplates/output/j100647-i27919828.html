<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
	<TITLE>$titleTag</TITLE>
<STYLE type="text/css">
   subtoolbar {border-top-width: 0px; border-top-style: none;}


   
 </STYLE>	
</head>
<body>
	
	<table width="940" height="496" border="0" align="center" cellpadding="0" cellspacing="0">
		<tr>
			<td height="489" align="right" valign="top"><SPAN><BR><INPUT type="BUTTON" name="BUTTON" value="< BACK" onclick="javascript: history.go(-1)" /></SPAN><BR>
			<table width="100%">
				<tr valign="top" align="left" style="color:#0066FF">
					<th width="33%">j100647</th>
					<th width="33%">i27919828</th>
					<th width="33%"><A HREF='http://phoenix.jstor.org/Phoenix/toc/secure/issue.html?workType=mod&journalId=10.2307/j100647&issueId=10.2307_i27919828' target='_blank'>PHX Link</A></th>
				</tr>
			</table>
			<P align='left' style='color:#993300'><B><u>10.2307/27919839</u></B></P><BR><P align='left' style='color:#CC66FF'><B><u>Current Issuemap</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>The introductory statistics course has traditionally trageted consumers of statistics with the intent of producing a citizenry capable of a critical analysis of basic published statistics. More recently, statistics educators have attempted to centre into course on real data, in part to motivate students and in part to create a more relevant course. The success of this approach is predicated on providing data that the students see as real and relevant. Modern students, however, have a different view of data than did students of 10 or even 5 years ago. Modern statistics courses must adjust to the fact that students' first exposure to data occurs outside the academy.</TD></TR></table><P align='left' style='color:#FF9900'><B><u>Most Eligible Issuemap Info</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>The introductory statistics course has traditionally trageted consumers of statistics with the intent of producing a citizenry capable of a critical analysis of basic published statistics. More recently, statistics educators have attempted to centre into course on real data, in part to motivate students and in part to create a more relevant course. The success of this approach is predicated on providing data that the students see as real and relevant. Modern students, however, have a different view of data than did students of 10 or even 5 years ago. Modern statistics courses must adjust to the fact that students' first exposure to data occurs outside the academy.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>2</TD><TD width='5%' bordercolor='blue'>Les cours d'initiation à la statistique on traditionellement visé les consomammateurs de la statistique avec l'intention de produire une population capable de faire une analyse critique des statistiques élémentaries publieés. Plus récemment, les professeurs de la statistique ont tenté d'orienter les cours d'initiation vers des données réelles, afin de motiver les élèves d'un part, et de créer un cours plus pertinent d'autre part. Le succès de cette approach repose sur une provision de données que les étudiants considèrent comme réels et pertinents. Cependent, les étudiants modernes ont une vision des données qui est différente de celle qu'ont eu les élèves d'il y a 10 ou même 5 ans. Les cours modernes de statistique doivent s'adapter au fait que la première rencontre des élèves aux données a lieu en dehors de l'académie.</TD></TR></table><P align='left' style='color:#993300'><B><u>10.2307/27919838</u></B></P><BR><P align='left' style='color:#CC66FF'><B><u>Current Issuemap</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>This paper seeks to add to a scholarly dialogue regarding the role and value of qualitative techniques in research on learning and using statistics. The paper briefly outlines some of the core assumptions of qualitative research methods, and present four examples to illustrate selected qualitative methods that are used by eductional researchers and service organizations. The discussion emphasizes the need to integrate quantitative and qualitative approaches in research on learners and users of statistics, and suggests that such integration may be needed to study emerging web-based communities of learners and users of statistics.</TD></TR></table><P align='left' style='color:#FF9900'><B><u>Most Eligible Issuemap Info</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>This paper seeks to add to a scholarly dialogue regarding the role and value of qualitative techniques in research on learning and using statistics. The paper briefly outlines some of the core assumptions of qualitative research methods, and present four examples to illustrate selected qualitative methods that are used by eductional researchers and service organizations. The discussion emphasizes the need to integrate quantitative and qualitative approaches in research on learners and users of statistics, and suggests that such integration may be needed to study emerging web-based communities of learners and users of statistics.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>2</TD><TD width='5%' bordercolor='blue'>Le présent article se propose d'apporter au dialogue scolaire relatif au rôle et à l'importance de la technique qualitative de la recherche sur l'apprentissage et l'utilisation des statistiques. En premier lieu, l'article décrit brièvement les notions de base des méthodes qualitatives de la recherche utilisées par les chercheurs dans le domaine de l'éducation ainsi que par les organisations de services. Par la suite, l'étude porte sur l'importance d'intégrer l'approche qualitative et l'approche quantitative dans de domaine des recherches sur les apprenants et utilisateurs des statistiques. En conclusion, l'étude propose l'intégration des deux approches, notamment pour étudier les communautés en ligne d'apprenants et d'utilisateurs des statistiques.</TD></TR></table><P align='left' style='color:#993300'><B><u>10.2307/27919837</u></B></P><BR><P align='left' style='color:#CC66FF'><B><u>Current Issuemap</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>In this paper, we highlight some qualitative facets of the discipline of statistics and argue that a qualitative approach, in particular a qualiative methodology known as phenomenography, allows us to research important aspects of statistics pedagogy. We summarize several components of our recent research into students' conceptions of statistics, their learning of statistics, our teaching of statistics, and their perceptions of their future professional work. We have obtained this information on the basis of analyses of several series of interviews with students studying statistics, both as statistics majors and as service students. In each of these cases, the broadest views relate in some way to personal connection, growth and change. In other words, they contain a strong ontological component-focusing on being or becoming a statistician-above and beyond the standard epistemological component-focusing on the knowledge required to do statistics. We discuss the importance of personal change in becoming a statistician, or an informed professional user of statistics, and investigate the pedagogical conditions under which such change is likely to occur.</TD></TR></table><P align='left' style='color:#FF9900'><B><u>Most Eligible Issuemap Info</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>In this paper, we highlight some qualitative facets of the discipline of statistics and argue that a qualitative approach, in particular a qualiative methodology known as phenomenography, allows us to research important aspects of statistics pedagogy. We summarize several components of our recent research into students' conceptions of statistics, their learning of statistics, our teaching of statistics, and their perceptions of their future professional work. We have obtained this information on the basis of analyses of several series of interviews with students studying statistics, both as statistics majors and as service students. In each of these cases, the broadest views relate in some way to personal connection, growth and change. In other words, they contain a strong ontological component-focusing on being or becoming a statistician-above and beyond the standard epistemological component-focusing on the knowledge required to do statistics. We discuss the importance of personal change in becoming a statistician, or an informed professional user of statistics, and investigate the pedagogical conditions under which such change is likely to occur.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>2</TD><TD width='5%' bordercolor='blue'>Dans cet article, nous mettons en évidence certaines propriétés qualitatives de la discipline des statistiques et suggérons qu'une approche qualitive, en particulier une méthodologie qualitive connue sous le nom de phénomengraphie, permet de rechercher des aspects importants de la pédagogie des statisques. Nous présentons un résumé de notre recherche récente sur les conceptions des étudiants en statistiques, leurs études des statistiques, notre enseignment des statistiques, et leurs perceptions de leur travail professionnel dans le future. Nous avons obtenu cette information sur la base d'analyses de plusieurs séries d'entrevues avec des étudiants en statistiques. Dans chacun des cas étudiés, les sentiments exprimés se rapportent a l'expérience, la croissance et aux changements personels. En d'autres terms, ils contiennent une large composante ontologique - se concentrant sur être ou devenir un stastisticien - au-dessus et au delà du composant épistémologique - se concentrant sur la connaissance exigée pour faire des statistiques. Nous discutons l'importance du changement personnel pour devenir un statisticien, ou un utilisateur professionnel compétent,et étudions les condition pédagogiques dans lesquelles un tel changement est susceptible de se produire.</TD></TR></table><P align='left' style='color:#993300'><B><u>10.2307/27919836</u></B></P><BR><P align='left' style='color:#CC66FF'><B><u>Current Issuemap</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>An important statistical application is the problem of determining an appropriate set of input variable for modelling a response variable. In such an application, candidate model are characterized by which input variable are included in the mean structure. A reasonable approach to gauging the propriety of a candidate model is to define a discrepancy function through the prediction error associated with this model. An optimal set of input variables is then determined by searching for the candidate model that minimizes the prediction error. In this paper, we focus on a Bayesian approach to estimating a discrepancy function based on prediction error in linear regression. It is shown how this approach provides an informative method for quantifying model selection uncertainty.</TD></TR></table><P align='left' style='color:#FF9900'><B><u>Most Eligible Issuemap Info</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>An important statistical application is the problem of determining an appropriate set of input variable for modelling a response variable. In such an application, candidate model are characterized by which input variable are included in the mean structure. A reasonable approach to gauging the propriety of a candidate model is to define a discrepancy function through the prediction error associated with this model. An optimal set of input variables is then determined by searching for the candidate model that minimizes the prediction error. In this paper, we focus on a Bayesian approach to estimating a discrepancy function based on prediction error in linear regression. It is shown how this approach provides an informative method for quantifying model selection uncertainty.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>2</TD><TD width='5%' bordercolor='blue'>La sélection des variable explicatives à prendre en considération dans la modélisation d' une réponse est un problème d'une grande importance pratique. Dans ce problème, les modèles-candidats sont caractérisés par l'ensemble des variable à inclure dans l'équation de régression. Une façon raisonnable d'évalure les mérites d'un modèle-candidat consiste à introduire une mesure de divergence fondée sur l'erreur de prédiction correspondante. Un ensemble de variables explicatives optimal est alors déterminé par minimisation de cette divergence. Dans cet article, nous nous concentrons sur une approche bayésienne de l'estimation de l'erreur de prédiction et de la mesure de divergence associée, dans un cadre de régression linéaire. Nous montrons comment cette approche permet une évaluation du risque lié à la sélection des variables.</TD></TR></table><P align='left' style='color:#993300'><B><u>10.2307/27919835</u></B></P><BR><P align='left' style='color:#CC66FF'><B><u>Current Issuemap</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>Many popular methods of model selection involve minimizing a penalized function of the data (such as the maximized log-likelihood or the residual sum of squares) over a set of models. The penalty in the criterion function is controlled by a penalty multiplier ? which determines the properties of the procedure. In this paper, we first review model selection criteria of the simple form “Loss + penalty” and then propose studying such model selection criteria as functions of the penalty multiplier. This approach can be interpreted as exploring the stability of model selection criteria through what we call model selection curves. It leads to new insights into model selection and new proposals on how to select models. We use the bootstrap to enhance the basic model selection curve and develop convenient numerical and graphical summaries of the results. The methodology is illustrated on two data sets and supported by a small simulation. We show that the new methodology can outperform methods such as AIC and BIC which correspond to single points on a model selection curve.</TD></TR></table><P align='left' style='color:#FF9900'><B><u>Most Eligible Issuemap Info</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>Many popular methods of model selection involve minimizing a penalized function of the data (such as the maximized log-likelihood or the residual sum of squares) over a set of models. The penalty in the criterion function is controlled by a penalty multiplier ? which determines the properties of the procedure. In this paper, we first review model selection criteria of the simple form “Loss + penalty” and then propose studying such model selection criteria as functions of the penalty multiplier. This approach can be interpreted as exploring the stability of model selection criteria through what we call model selection curves. It leads to new insights into model selection and new proposals on how to select models. We use the bootstrap to enhance the basic model selection curve and develop convenient numerical and graphical summaries of the results. The methodology is illustrated on two data sets and supported by a small simulation. We show that the new methodology can outperform methods such as AIC and BIC which correspond to single points on a model selection curve.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>2</TD><TD width='5%' bordercolor='blue'>Beaucoup de méthodes populaires de sélection de variables impliquent la minimisation d'une fonction pénalisée des données (comme la vraisemblance maximisée ou la somme résiduelle carrés) sur un jeu de modèles. La pénalité dans la fonction de critère est contrôlée par un multiplicateur de pénalité ? qui détermine les propriétés de la procédure. Nous reconsidérons d'abord des critères de sélection modèles de la forme simple 'Perte + Pénalité' et proposons ensuite d'étudier de telles fonctions comme les fonctions du multiplicateur de pénalité. Cette approache peut être interprétée comme l'exploration de la stabilité de fonctions de critère par ce que nous appelons des courbes de choix modèles. Il mène à de nouvelles compréhensions dans le sélection de variables et de nouvelles propositions de la façon d'utiliser ces fonctions de critère pour sélectionner de variables. Nous utilisons le bootstrap pour augmenter des courbes de choix modèle et développent les résumés numériques et graphiques des résulants. La méthodologie est illustrée sur deux jeux de données et soutenue par une petite simulation. Nous montrons que la nouvelle méthodologie peut surpasser des méthodes comme AIC et BIC qui correspond aux points simples sur une courbe de choix modèle.</TD></TR></table><P align='left' style='color:#993300'><B><u>10.2307/27919834</u></B></P><BR><P align='left' style='color:#CC66FF'><B><u>Current Issuemap</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>Intervention analysis has been recently the subject of several studies, mainly because real time series present a wide variety of phenomena that are caused by external and/or unexpected events. In this work, transfer functions are used to model different forms of invention to the mean level of a time series. This is performed in the framework of state-space models. Two canonical forms of intervention are considered: pulse and step functions. Static and dynamic explanation of the intervention effects, normal and non-normal time series, detection of intervention, and study of the effect of outliers are also discussed. The performance of the two approaches is compared in terms of point and interval estimation through Monte Carlo Simulation. The methodology was applied to real time series and showed satisfactory results for the intervention models used.</TD></TR></table><P align='left' style='color:#FF9900'><B><u>Most Eligible Issuemap Info</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>Intervention analysis has been recently the subject of several studies, mainly because real time series present a wide variety of phenomena that are caused by external and/or unexpected events. In this work, transfer functions are used to model different forms of invention to the mean level of a time series. This is performed in the framework of state-space models. Two canonical forms of intervention are considered: pulse and step functions. Static and dynamic explanation of the intervention effects, normal and non-normal time series, detection of intervention, and study of the effect of outliers are also discussed. The performance of the two approaches is compared in terms of point and interval estimation through Monte Carlo Simulation. The methodology was applied to real time series and showed satisfactory results for the intervention models used.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>2</TD><TD width='5%' bordercolor='blue'>L'analyse d'invention a récemment fait l'object de plusieurs études, principalement parce que les séries chronologiques réel présentent une grande variéte de phénomènes qui sont causés par des événements extrnes et/ou inattendus. Dans ce travail, les fonctions de transfer sont employées pour modéliser différentes formes d'intervention au niveau moyen d'une série temporelle. Cette tâche est réalisée dans le cadre de modèles de état-space. Deux formes canoniques d'intervention sont prises en considération: les fonctions d'impulsion et d'étape. Les modèles considérés tiennent ègalement compte de l'explication statique et dynamique des effets d'intervention, séries chronologiques normales et non-normales, détection de l'intervention et l'étude des effect des valeurs aberrantes. La comparasion entre les deux approches est effectuée en termes d'estimation ponctuelle et d'intervalle par simulation de Monte Carlo. La méthodologie a été appliquée à séries chronologiques réel a donné des résultats satisfaisants pour les modèles d'intervention utilisés.</TD></TR></table><P align='left' style='color:#993300'><B><u>10.2307/27919832</u></B></P><BR><P align='left' style='color:#CC66FF'><B><u>Current Issuemap</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>Several anomalies in the foundations of ridge regression from the perspective of constrained least-square (LS) problems were pointed out in Jensen & Ramirez. Some of these so-called anomalies, attributed to the non-monotonic behaviour of the norm of unconstrained ridge estimators and the consequent lack of sufficency of Lagrange's principle, are shown to be incorrect. It is noted in this paper that, for a fixed Y, norms of unconstrained ridge estimators corresponding to the given basis are indeed strictly monotone. Furthermore, the conditions for sufficiency of Lagrange's principle are valid for a suitable range of the constraint parameter. The discrepancy arose in the context of one data set due to confusion between estimates of the parameter vector, ?, corresponding to different parametrization (choice of bases) and/or constraint norms. In order to avoid such confusion, it is suggested that the parameter ? corresponding to each basis be labelled appropriately.</TD></TR></table><P align='left' style='color:#FF9900'><B><u>Most Eligible Issuemap Info</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>Several anomalies in the foundations of ridge regression from the perspective of constrained least-square (LS) problems were pointed out in Jensen & Ramirez. Some of these so-called anomalies, attributed to the non-monotonic behaviour of the norm of unconstrained ridge estimators and the consequent lack of sufficency of Lagrange's principle, are shown to be incorrect. It is noted in this paper that, for a fixed Y, norms of unconstrained ridge estimators corresponding to the given basis are indeed strictly monotone. Furthermore, the conditions for sufficiency of Lagrange's principle are valid for a suitable range of the constraint parameter. The discrepancy arose in the context of one data set due to confusion between estimates of the parameter vector, ?, corresponding to different parametrization (choice of bases) and/or constraint norms. In order to avoid such confusion, it is suggested that the parameter ? corresponding to each basis be labelled appropriately.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>2</TD><TD width='5%' bordercolor='blue'>Plusieurs anomalies ont été récemment relevées par Jensen et Ramirez (2008) dans les fondements théoriques de la “ridege regression” considérée dans une perpective de moindres carrés constraints. Certaines de ces anomalies ont été attribuées au comportement non monotone de la norme des “ridge-estimateurs” non contraints, ainsi qu'au caractère non suffisiant du principe de Lagrange. Nous indiquons dans cet article que, pour une valeur fixée de Y, la norme des ridge-estimateurs correspondant à une base donnée sont strictement monotones. En outre, les conditions assurant le caractère suffisant du principe de Lagrange sont satisfaites pour un ensemble adéquat de valeurs du paramètre contraint. L'origine des anomalies relevées se trouve done ailleurs. Cette apparente contradiction prend son origine, dans le contexte de l'étude d'un ensemble de données particulier, dans la confusion entre les estimateurs du vecteur de paramètres ? correspondant à différentes paramétrisations (associées à différents choix d'une base)et/ou à differentes normes. Afin d'éviter ce type de confusion, il est suggéré d'indexer le paramètre de façon adéquate au moyen de la base choisie.</TD></TR></table><P align='left' style='color:#993300'><B><u>10.2307/27919831</u></B></P><BR><P align='left' style='color:#CC66FF'><B><u>Current Issuemap</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>This paper deals with linear state space modelling subject to general linear constraints on the state vector. The discussion concentrates on four topics: the constrained Kalman filtering versus the recursive restricted least squares estimator; a new proof of the constrained Kalman filtering under a conditional expectation framework; linear constraints under a reduced state space modelling; and state vector prediction under linear constraints. The techniques proposed are illustrated in two real problems. The first problem is related to investment analysis under a dynamic factor model, whereas the second is about making constrained predictions within a GDP benchmaking estimation.</TD></TR></table><P align='left' style='color:#FF9900'><B><u>Most Eligible Issuemap Info</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>This paper deals with linear state space modelling subject to general linear constraints on the state vector. The discussion concentrates on four topics: the constrained Kalman filtering versus the recursive restricted least squares estimator; a new proof of the constrained Kalman filtering under a conditional expectation framework; linear constraints under a reduced state space modelling; and state vector prediction under linear constraints. The techniques proposed are illustrated in two real problems. The first problem is related to investment analysis under a dynamic factor model, whereas the second is about making constrained predictions within a GDP benchmaking estimation.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>2</TD><TD width='5%' bordercolor='blue'>Cet article traite des modèles espace-état sujets aux restrictions linéaires générales sur le vecteur d'état. La discussion se concentre autour de quatre aspects: le filtrage de Kalman restreint versus l'estimateur de moindres carrés restreint recursive; une nouvelle preuve du filtrage de Kalman restreint sous le cadre de l'espérance conditionelle; restrictions linéaires aux modèles espace-état réduits; et la prédiction d'état sous restrictions linéaires. Les techniques proposées sont illustrées par deux problèmes réels. Le premier problème est concerné par l'analyse d'investissement sous un modèle à facteur dynamique, tandis que le second concerne les prédictions restreintes dans l'estimation de benchmarking.</TD></TR></table><P align='left' style='color:#993300'><B><u>10.2307/27919830</u></B></P><BR><P align='left' style='color:#CC66FF'><B><u>Current Issuemap</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>At first sight, web surveys seem to be an interesting and attractive means of data collection. They provide simple, cheap, and fast access to a large group of potential respondents. However, web surveys are not without methodological problems. Specific groups in the populations are under-respresented because they have less access to Internet. Furthermore, recruitment of respondents is often based on self-selection. Both under-coverage and self-selection may lead to biased estimates. This paper describes these methodological problems. It also explores the effect of various correction techniques (adjustment weighting and use of reference surveys). This all leads to the question whether properly design web surveys can be used for data collection. The paper attempts to answer this question. It concludes that under-coverage problems may solve itself in the future, but that self-selection leads to unreliable survey outcomes.</TD></TR></table><P align='left' style='color:#FF9900'><B><u>Most Eligible Issuemap Info</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>At first sight, web surveys seem to be an interesting and attractive means of data collection. They provide simple, cheap, and fast access to a large group of potential respondents. However, web surveys are not without methodological problems. Specific groups in the populations are under-respresented because they have less access to Internet. Furthermore, recruitment of respondents is often based on self-selection. Both under-coverage and self-selection may lead to biased estimates. This paper describes these methodological problems. It also explores the effect of various correction techniques (adjustment weighting and use of reference surveys). This all leads to the question whether properly design web surveys can be used for data collection. The paper attempts to answer this question. It concludes that under-coverage problems may solve itself in the future, but that self-selection leads to unreliable survey outcomes.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>2</TD><TD width='5%' bordercolor='blue'>A première vue, les enquêtes en ligne semble être un moyen intéressant et attrayant de collecte de données. Ils fournissent un accès simple, rapide et peu coûteux à un grand groupe de répondants potentiels. Les enquêtes en ligne ne sont toutefois pas sans problèmes méthodologiques. Certains groupes au sein des populations sont sous-représentés parce qu'ils ont un accès restreint à Internet. En outre, le recrutement des personnes interrogées est souvent fondé sur l'auto-sélection. Sous-couverture et auto-sélection peuvent toutes deux être la source d' estimations biaisées. Le présent article décrit ces problèmes méthodologiques. Il explore également l'effet des différentes techniques de correction (pondération corrective et utilisation d'enquêtes de référence). Tout ceci amène à la question de savoir si des enquêtes en ligne bien conçues sont propres à être utilisées pour la collecte de données. L'article tente de répondre à cette question. Il conclut que le problème de la sous-couverture pourrait se résoudre dans l'avenir, mais que l'auto-sélection aboutit à des résultats d'enquête non fiables.</TD></TR></table>
				
		</td>
		</tr>
	</table>		
</body>
</html>
