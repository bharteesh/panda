<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
	<TITLE>$titleTag</TITLE>
<STYLE type="text/css">
   subtoolbar {border-top-width: 0px; border-top-style: none;}


   
 </STYLE>	
</head>
<body>
	
	<table width="940" height="496" border="0" align="center" cellpadding="0" cellspacing="0">
		<tr>
			<td height="489" align="right" valign="top"><SPAN><BR><INPUT type="BUTTON" name="BUTTON" value="< BACK" onclick="javascript: history.go(-1)" /></SPAN><BR>
			<table width="100%">
				<tr valign="top" align="left" style="color:#0066FF">
					<th width="33%">j100647</th>
					<th width="33%">i27919607</th>
					<th width="33%"><A HREF='http://phoenix.jstor.org/Phoenix/toc/secure/issue.html?workType=mod&journalId=10.2307/j100647&issueId=10.2307_i27919607' target='_blank'>PHX Link</A></th>
				</tr>
			</table>
			<P align='left' style='color:#993300'><B><u>10.2307/27919611</u></B></P><BR><P align='left' style='color:#CC66FF'><B><u>Current Issuemap</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>This work presents a method for estimating trends of economic time series that allows the user to fix at the outset the desired percentage of smoothness for the trend. The calculations are based on the Hodrick-Prescott (HP) filter usually employed in business cycle analysis. The situation considered here is not related to that kind of analysis, but with describing the dynamic behaviour of the series by way of a smooth curve. To apply the filter, the user has to specify a smoothing constant that determines the dynamic behaviour of the trend. A new method that formalizes the concept of trend smoothness is proposed here to choose that constant. Smoothness of the trend is measured in percentage terms with the aid of an index related to the underlying statistical model of the HP filter. Empirical illustrations are provided using data on Mexico's GDP.</TD></TR></table><P align='left' style='color:#FF9900'><B><u>Most Eligible Issuemap Info</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>This work presents a method for estimating trends of economic time series that allows the user to fix at the outset the desired percentage of smoothness for the trend. The calculations are based on the Hodrick-Prescott (HP) filter usually employed in business cycle analysis. The situation considered here is not related to that kind of analysis, but with describing the dynamic behaviour of the series by way of a smooth curve. To apply the filter, the user has to specify a smoothing constant that determines the dynamic behaviour of the trend. A new method that formalizes the concept of trend smoothness is proposed here to choose that constant. Smoothness of the trend is measured in percentage terms with the aid of an index related to the underlying statistical model of the HP filter. Empirical illustrations are provided using data on Mexico's GDP.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>2</TD><TD width='5%' bordercolor='blue'>Ce travail présente un méthode pour estimer les tendances des séries de temps économiques qui permet à l'usager fixer dès début le pourcentage désiré de douceur pour la tendance. Les calculs ont fondement en le filtre de Hodrick et Prescott que s'emploie généralement dans l'analyse de cycles économiques. La situation ici considéré n'a pas relation avec ce type d'analyse, mais comment la description du comportement dynamique des séries avec une courbe douce. Pour appliquer le filtre, l'usager a besoin de spécifier une constante de douceur que détermine le comportement dynamique de la tendance. Un nouveau méthode que formalise le concept de douceur de la tendance est ici proposé pour choisir la constante. La douceur de la tendance est mesuré en termes de pourcentage avec l'aide d'un index rapporté avec le modèle statistique après le filtre. Quelques illustrations empiriques sont munies avec données de l'économie mexicaine.</TD></TR></table><P align='left' style='color:#993300'><B><u>10.2307/27919612</u></B></P><BR><P align='left' style='color:#CC66FF'><B><u>Current Issuemap</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>We discuss the hypothesis of bivariate exchangeability and show that testing bivariate exchangeability is related to the two-sample testing of equality of distribution functions. We consider three test statistics based on the ordering of the Euclidean interpoint distances. The runs test of exchangeability counts the runs among the observations and their mirror images on the minimal spanning tree. The nearest neighbour test of exchangeability is based on the number of nearest neighbour type coincidences among the observations and their folded images on the plane. The rank test of exchangeability compares the within and between ranks of the interpoint distances. We also consider the sign test of exchangeability, which uses the signs of the observations in specific regions, and a bootstrap test of exchangeability based on the maximum distance between the mirror images. We compare the power of these methods in a Monte Carlo study which shows different power orderings of the methods, depending on the alternative hypothesis.</TD></TR></table><P align='left' style='color:#FF9900'><B><u>Most Eligible Issuemap Info</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>We discuss the hypothesis of bivariate exchangeability and show that testing bivariate exchangeability is related to the two-sample testing of equality of distribution functions. We consider three test statistics based on the ordering of the Euclidean interpoint distances. The runs test of exchangeability counts the runs among the observations and their mirror images on the minimal spanning tree. The nearest neighbour test of exchangeability is based on the number of nearest neighbour type coincidences among the observations and their folded images on the plane. The rank test of exchangeability compares the within and between ranks of the interpoint distances. We also consider the sign test of exchangeability, which uses the signs of the observations in specific regions, and a bootstrap test of exchangeability based on the maximum distance between the mirror images. We compare the power of these methods in a Monte Carlo study which shows different power orderings of the methods, depending on the alternative hypothesis.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>2</TD><TD width='5%' bordercolor='blue'>Nous discutons le problème du test de l'hypothèse d'échangeabilité bivariée, en liaison avec celui de l'hypothèse d'égalité de deux fonctions de répartition. Nous considérons trois types de statistiques de test fondées sur les distances euclidiennes entre les observations et leurs symétriques. Les tests de séquences dénombrent les séquences dans les suites formées par les observations et leurs images-miroir dans l'arbre générateur minimal. Les tests de plus proches voisins considèrent le cardinal de l'intersection des ensembles de plus proches voisins dans l'ensemble formé par les observations et leurs images-miroir. Les tests de rangs sont fondés sur les rangs des distances euclidiennes entre les observations. Nous prenons aussi en considération un test de signes dans lequel les signes sont ceux des observations appartenant à certaines régions spécifiques, et un test de type rééchantillonnage fondé sur les distances maximales entre les observations et leur image-miroir. Une étude par simulation des puissances comparées de ces divers tests est présentée et commentée.</TD></TR></table><P align='left' style='color:#993300'><B><u>10.2307/27919610</u></B></P><BR><P align='left' style='color:#CC66FF'><B><u>Current Issuemap</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>We detail a method of simulating data from long range dependent processes with variance-gamma or t distributed increments, test various estimation procedures [method of moments (MOM), product-density maximum likelihood (PMLE), non-standard minimum ?
              <sup>2</sup>
              and empirical characteristic function estimation] on the data, and assess the performance of each. The investigation is motivated by the apparent poor performance of the MOM technique using real data (Tjetjep & Seneta, 2006); and the need to assess the performance of PMLE for our dependent data models. In the simulations considered the product-density method performs favourably.</TD></TR></table><P align='left' style='color:#FF9900'><B><u>Most Eligible Issuemap Info</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>We detail a method of simulating data from long range dependent processes with variance-gamma or t distributed increments, test various estimation procedures [method of moments (MOM), product-density maximum likelihood (PMLE), non-standard minimum ? and empirical characteristic function estimation] on the data, and assess the performance of each. The investigation is motivated by the apparent poor performance of the MOM technique using real data (Tjetjep & Seneta, 2006); and the need to assess the performance of PMLE for our dependent data models. In the simulations considered the product-density method performs favourably.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>2</TD><TD width='5%' bordercolor='blue'>Nous détaillons une méthode de simulation de données relatives à des processus à accroissements de lois Variance-Gamma ou t. Nous testons sur ces données diverses procédures d'estimation (méthode des moments, maximum de vraisemblance, ? non standard minimum, et fonction caractéristique empirique) et nous évaluons la performance de chacune. Cette étude est motivée par le peu d'efficacité de la technique des moments appliquée à des données réelles (Tjetjep et Seneta 2006) et par le besoin d'évaluer la performance de la méthode du maximum de vraisemblance relative à une densité produit appliquée à nos modéles de données dépendantes. Dans les simulations que nous avons faites la méthode de la densité produit donne des résultats satisfaisants.</TD></TR></table><P align='left' style='color:#993300'><B><u>10.2307/27919615</u></B></P><BR><P align='left' style='color:#CC66FF'><B><u>Current Issuemap</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>The state of the art in coherent structure theory is driven by two assertions, both of which are limiting: (1) all units of a system can exist in one of two states, failed or functioning; and (2) at any point in time, each unit can exist in only one of the above states. In actuality, units can exist in more than two states, and it is possible that a unit can simultaneously exist in more than one state. This latter feature is a consequence of the view that it may not be possible to precisely define the subsets of a set of states; such subsets are called vague. The first limitation has been addressed via work labeled 'multistate systems'; however, this work has not capitalized on the mathematics of many-valued propositions in logic. Here, we invoke its truth tables to define the structure function of multistate systems and then harness our results in the context of vagueness. A key contribution of this paper is to argue that many-valued logic is a common platform for studying both multistate and vague systems but, to do so, it is necessary to lean on several principles of statistical inference.</TD></TR></table><P align='left' style='color:#FF9900'><B><u>Most Eligible Issuemap Info</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>The state of the art in coherent structure theory is driven by two assertions, both of which are limiting: (1) all units of a system can exist in one of two states, failed or functioning; and (2) at any point in time, each unit can exist in only one of the above states. In actuality, units can exist in more than two states, and it is possible that a unit can simultaneously exist in more than one state. This latter feature is a consequence of the view that it may not be possible to precisely define the subsets of a set of states; such subsets are called vague. The first limitation has been addressed via work labeled 'multistate systems'; however, this work has not capitalized on the mathematics of many-valued propositions in logic. Here, we invoke its truth tables to define the structure function of multistate systems and then harness our results in the context of vagueness. A key contribution of this paper is to argue that many-valued logic is a common platform for studying both multistate and vague systems but, to do so, it is necessary to lean on several principles of statistical inference.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>2</TD><TD width='5%' bordercolor='blue'>Létat de l'art dans la théorie de structure cohérente est guidé par deux assertions qui sont tous deux limitants: (1) toutes les unités d'un systéme peuvent exister dans un de deux états, défaillant ou fonctionnant; et (2) à n'importe quel moment, chaque unité peut seulement exister dans un des susdits états. En réalite, les unités peuvent exister dans plus de deux états et c'est possible qu'une unité puisse simultanément exister dans plus d'un état. Cette derniére caractéristique est une conséquence de l'opinion qu'il ne soit peut-être pas possible de définir avec précision les sous-ensembles d'un ensemble d'états; on appelle de tels sous-ensembles vagues. La premiére restriction a été adressée par les méthodes appelées "systémes multi-états"; pourtant, ces méthodes n'ont pas pris avantage des mathématiques sur les propositions multivalues en logique. Ici, nous invoquons ses tables de vérité pour définir la fonction des systémes multi-états et exploiter ensuite nos résultats dans le contexte d'ambiguïté. Une contribution clé de ce papier est d'argumenter que la logique de plusieurs values est une plateforme commune pour étudier tant les systemes multi-états que les systémes vagues, mais pour faire ceci, il est nécessaire de se baser sur plusieurs principes d'inférence statistique.</TD></TR></table><P align='left' style='color:#993300'><B><u>10.2307/27919616</u></B></P><BR><P align='left' style='color:#CC66FF'><B><u>Current Issuemap</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>A Bayesian hierarchical mixed model is developed for multiple comparisons under a simple order restriction. The model facilitates inferences on the successive differences of the population means, for which we choose independent prior distributions that are mixtures of an exponential distribution and a discrete distribution with its entire mass at zero. We employ Markov Chain Monte Carlo (MCMC) techniques to obtain parameter estimates and estimates of the posterior probabilities that any two of the means are equal. The latter estimates allow one both to determine if any two means are significantly different and to test the homogeneity of all of the means. We investigate the performance of the model-based inferences with simulated data sets, focusing on parameter estimation and successive-mean comparisons using posterior probabilities. We then illustrate the utility of the model in an application based on data from a study designed to reduce lead blood concentrations in children with elevated levels. Our results show that the proposed hierarchical model can effectively unify parameter estimation, tests of hypotheses and multiple comparisons in one setting.</TD></TR></table><P align='left' style='color:#FF9900'><B><u>Most Eligible Issuemap Info</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>A Bayesian hierarchical mixed model is developed for multiple comparisons under a simple order restriction. The model facilitates inferences on the successive differences of the population means, for which we choose independent prior distributions that are mixtures of an exponential distribution and a discrete distribution with its entire mass at zero. We employ Markov Chain Monte Carlo (MCMC) techniques to obtain parameter estimates and estimates of the posterior probabilities that any two of the means are equal. The latter estimates allow one both to determine if any two means are significantly different and to test the homogeneity of all of the means. We investigate the performance of the model-based inferences with simulated data sets, focusing on parameter estimation and successive-mean comparisons using posterior probabilities. We then illustrate the utility of the model in an application based on data from a study designed to reduce lead blood concentrations in children with elevated levels. Our results show that the proposed hierarchical model can effectively unify parameter estimation, tests of hypotheses and multiple comparisons in one setting.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>2</TD><TD width='5%' bordercolor='blue'>Un modèle Bayésien hiérarchique mixte est développé pour des comparaisons multiples avec une simple restriction d'ordre. Le modèle facilite les inférences sur les différences successives des moyennes de population, pour lesquelles nous choisissons des distributions indépendantes préalables qui sont des mélanges d'une distribution exponentielle et d'une distribution discrète avec sa masse entière à zéro. Nous employons les techniques de la chaîne de Markov Monte Carlo pour obtenir des estimations des paramètres et des estimations des probabilités postérieures que deux quelconques des moyennes sont égales. Les seconds estimateurs permettent à chacun de déterminer si deux quelconques des moyennes sont significativement différentes et de tester l'homogénéité de toutes les moyennes. Nous étudions la performance des inférences basées sur des modèles avec des jeux de données simulées, en se concentrant sur l'estimation du paramètre et des comparaisons successives moyennes utilisant des probabilités postérieures. Nous illustrons ensuite l'utilité du modèle dans une application basée sur les données d'une étude destinée à réduire les concentrations de plomb sanguin chez les enfants avec des niveaux élevés. Nos résultats montrent que le modélé hiérarchique proposé peut efficacement unifier l'estimation des paramètres, les hypothèses de tests et les comparaisons multiples dans un seul cadre.</TD></TR></table><P align='left' style='color:#993300'><B><u>10.2307/27919613</u></B></P><BR><P align='left' style='color:#CC66FF'><B><u>Current Issuemap</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>The aim of this paper is to convey to a wider audience of applied statisticians that nonparametric (matching) estimation methods can be a very convenient tool to overcome problems with endogenous control variables. In empirical research one is often interested in the causal effect of a variable X on some outcome variable Y. With observational data, i.e. in the absence of random assignment, the correlation between X and Y generally does not reflect the treatment effect but is confounded by differences in observed and unobserved characteristics. Econometricians often use two different approaches to overcome this problem of confounding by other characteristics. First, controlling for observed characteristics, often referred to as selection on observables, or instrumental variables regression, usually with additional control variables. Instrumental variables estimation is probably the most important estimator in applied work. In many applications, these control variables are themselves correlated with the error term, making ordinary least squares and two-stage least squares inconsistent. The usual solution is to search for additional instrumental variables for these endogenous control variables, which is often difficult. We argue that nonparametric methods help to reduce the number of instruments needed. In fact, we need only one instrument whereas with conventional approaches one may need two, three or even more instruments for consistency. Nonparametric matching estimators permit ?n consistent estimation without the need for (additional) instrumental variables and permit arbitrary functional forms and treatment effect heterogeneity.</TD></TR></table><P align='left' style='color:#FF9900'><B><u>Most Eligible Issuemap Info</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>The aim of this paper is to convey to a wider audience of applied statisticians that nonparametric (matching) estimation methods can be a very convenient tool to overcome problems with endogenous control variables. In empirical research one is often interested in the causal effect of a variable X on some outcome variable Y. With observational data, i.e. in the absence of random assignment, the correlation between X and Y generally does not reflect the treatment effect but is confounded by differences in observed and unobserved characteristics. Econometricians often use two different approaches to overcome this problem of confounding by other characteristics. First, controlling for observed characteristics, often referred to as selection on observables, or instrumental variables regression, usually with additional control variables. Instrumental variables estimation is probably the most important estimator in applied work. In many applications, these control variables are themselves correlated with the error term, making ordinary least squares and two-stage least squares inconsistent. The usual solution is to search for additional instrumental variables for these endogenous control variables, which is often difficult. We argue that nonparametric methods help to reduce the number of instruments needed. In fact, we need only one instrument whereas with conventional approaches one may need two, three or even more instruments for consistency. Nonparametric matching estimators permit ?n consistent estimation without the need for (additional) instrumental variables and permit arbitrary functional forms and treatment effect heterogeneity.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>2</TD><TD width='5%' bordercolor='blue'>Cet article démontre que l'estimation non paramétrique peut être utile pour résoudre le problème lié aux variables de contrôle endogènes. L'objectif de nombreux travaux empiriques est d'identifier l'effet causal d'une variable X sur une variable dépendante Y. La corrélation entre X et Y qui est observée dans les données ne reflète généralement pas l'effet du traitement car celui-ci est masqué par les différences dans les caractéristiques (observables ou non) des deux groupes. Les économètres résolvent souvent ce problème d'une des deux façons suivantes: (1) en contrôlant pour la sélection qui est liée aux caractéristiques observées ou (2) en utilisant des instruments, qui ne sont fréquemment valides que conditionnellement à d'autres variables de contrôle. L'estimation basée sur des instruments (IV) est probablement la méthode la plus importante dans la recherche appliquée. Dans beaucoup d'applications ces variables de contrôle sont elles-mêmes suspectées d'endogénéité ce qui rendrait OLS et 2SLS inconsistants. La solution habituelle est de chercher des instruments supplémentaires pour ces variables de contrôle endogènes, mais cette approche est très difficile en pratique. Nous montrons dans cet article qu'utiliser une méthode instrumentale non paramétrique réduit le nombre des instruments nécessaires. En effet, nous n'avons besoin dans ce cas que d'un seul instrument alors que les méthodes conventionnelles nécessitent deux, trois ou plus encore d'instruments pour garantir leur consistance. Il existe des estimateurs non paramétriques basés sur le matching qui convergent à la vitesse racine de n sans exiger des instruments supplémentaires et qui ne restreignent ni la forme fonctionnelle ni l'hetérogénéité de l'effet du traitement.</TD></TR></table><P align='left' style='color:#993300'><B><u>10.2307/27919614</u></B></P><BR><P align='left' style='color:#CC66FF'><B><u>Current Issuemap</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>One frequent application of microarray experiments is in the study of monitoring gene activities in a cell during cell cycle or cell division. High throughput gene expression time series data are produced from such microarray experiments. A new computational and statistical challenge for analyzing such gene expression time course data, resulting from cell cycle microarray experiments, is to discover genes that are statistically significantly periodically expressed during the cell cycle. Such a challenge occurs due to the large number of genes that are simultaneously measured, a moderate to small number of measurements per gene taken at different time points and high levels of non-normal random noises inherited in the data. Computational and statistical approaches to discovery and validation of periodic patterns of gene expression are, however, very limited. A good method of analysis should be able to search for significant periodic genes with a controlled family-wise error (FWE) rate or controlled false discovery rate (FDR) and any other variations of FDR, when all gene expression profiles are compared simultaneously. In this review paper, a brief summary of currently used methods in searching for periodic genes will be given. In particular, two methods will be surveyed in details. The first one is a novel statistical inference approach, the C & G Procedure that can be used to effectively detect statistically significantly periodically expressed genes when the gene expression is measured on evenly spaced time points. The second one is the Lomb–Scargle periodogram analysis, which can be used to discover periodic genes when the gene profiles are not measured on evenly spaced time points or when there are missing values in the profiles. The ultimate goal of this review paper is to give an expository of the two surveyed methods to researchers in related fields.</TD></TR></table><P align='left' style='color:#FF9900'><B><u>Most Eligible Issuemap Info</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>One frequent application of microarray experiments is in the study of monitoring gene activities in a cell during cell cycle or cell division. High throughput gene expression time series data are produced from such microarray experiments. A new computational and statistical challenge for analyzing such gene expression time course data, resulting from cell cycle microarray experiments, is to discover genes that are statistically significantly periodically expressed during the cell cycle. Such a challenge occurs due to the large number of genes that are simultaneously measured, a moderate to small number of measurements per gene taken at different time points and high levels of non-normal random noises inherited in the data. Computational and statistical approaches to discovery and validation of periodic patterns of gene expression are, however, very limited. A good method of analysis should be able to search for significant periodic genes with a controlled family-wise error (FWE) rate or controlled false discovery rate (FDR) and any other variations of FDR, when all gene expression profiles are compared simultaneously. In this review paper, a brief summary of currently used methods in searching for periodic genes will be given. In particular, two methods will be surveyed in details. The first one is a novel statistical inference approach, the C & G Procedure that can be used to effectively detect statistically significantly periodically expressed genes when the gene expression is measured on evenly spaced time points. The second one is the Lomb–Scargle periodogram analysis, which can be used to discover periodic genes when the gene profiles are not measured on evenly spaced time points or when there are missing values in the profiles. The ultimate goal of this review paper is to give an expository of the two surveyed methods to researchers in related fields.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>2</TD><TD width='5%' bordercolor='blue'>Une application fréquente des expériences de microréseaux se trouve dans l'étude du suivi des activités du gène pendant le cycle cellulaire ou la division cellulaire.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>3</TD><TD width='5%' bordercolor='blue'>Des séries temporelles d'expression de gènes à haut débit sont produites à partir de telles expériences.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>4</TD><TD width='5%' bordercolor='blue'>Un nouveau défi, informatique et statistique, pour analyser de telles données temporelles d'expression génétique, résultant des expériences de microréseaux du cycle cellulaire, est de découvrir les gènes qui, statistiquement et significativement, sont exprimés périodiquement durant le cycle cellulaire.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>5</TD><TD width='5%' bordercolor='blue'>Un tel défi apparaît en raison du grand nombre de gènes mesurés simultanément, d'un nombre modéré à faible de mesures par gène prises à différents moments et de hauts niveaux de bruits aléatoires non normaux dans les données.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>6</TD><TD width='5%' bordercolor='blue'>Les approches quantitatives et statistiques pour la découverte et la validation des modèles périodiques d'expression génétique sont cependant très limitées. Une bonne méthode d'analyse devrait être capable de rechercher des gènes périodiques significatifs avec un taux d'erreur par famille (FWE) contrôlé, ou un taux de fausse découverte (FDR) contrôlé et d'autres variations de ce taux, quand tous les profils d'expression génétique sont comparés simultanément.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>7</TD><TD width='5%' bordercolor='blue'>Dans cet article récapitulatif, un bref résumé des méthodes actuellement utilisées pour la recherche des gènes périodiques sera donné. En particulier, deux méthodes seront analysées en détail. La première est une approche originale d'inférence statistique, la procédure C&G qui peut être utilisée pour détecter efficacement des gènes exprimés périodiquement, statistiquement et significativement, quand l'expression du gène est mesurée à des moments espacés uniformément. La seconde est l'analyse par périodogramme de Lomb-Scargle, qui peut être utilisée pour découvrir des gènes périodiques quand les profils génétiques ne sont pas mesurés à des moments régulièrement espacés ou quand il y a des valeurs manquantes dans les profils. Le dernier objectif de cet article est d'exposer les deux méthodes aux chercheurs dans les champs concernés.</TD></TR></table><P align='left' style='color:#993300'><B><u>10.2307/27919617</u></B></P><BR><P align='left' style='color:#CC66FF'><B><u>Current Issuemap</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>Whether doing parametric or nonparametric regression with shrinkage, thresholding, penalized likelihood, Bayesian posterior estimators (e.g., ridge regression, lasso, principal component regression, waveshrink or Markov random field), it is common practice to rescale covariates by dividing by their respective standard errors ?. The stated goal of this operation is to provide unitless covariates to compare like with like, especially when penalized likelihood or prior distributions are used. We contend that this vision is too simplistic. Instead, we propose to take into account a more essential component of the structure of the regression matrix by rescaling the covariates based on the diagonal elements of the covariance matrix ? of the maximum-likelihood estimator. We illustrate the differences between the standard ?- and proposed ?-rescalings with various estimators and data sets.</TD></TR></table><P align='left' style='color:#FF9900'><B><u>Most Eligible Issuemap Info</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>Whether doing parametric or nonparametric regression with shrinkage, thresholding, penalized likelihood, Bayesian posterior estimators (e.g., ridge regression, lasso, principal component regression, waveshrink or Markov random field), it is common practice to rescale covariates by dividing by their respective standard errors ?. The stated goal of this operation is to provide unitless covariates to compare like with like, especially when penalized likelihood or prior distributions are used. We contend that this vision is too simplistic. Instead, we propose to take into account a more essential component of the structure of the regression matrix by rescaling the covariates based on the diagonal elements of the covariance matrix ? of the maximum-likelihood estimator. We illustrate the differences between the standard ?- and proposed ?-rescalings with various estimators and data sets.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>2</TD><TD width='5%' bordercolor='blue'>Que l'on utilise un modèle de régression paramétrique ou non-paramétrique, par rétrécissement, seuillage, vraisemblance pénalisée ou Bayesien (ex. régression ridge, lasso, régression en composantes principales, waveshrink, champ Markovien), il est commun de standardiser les variables explicatives en les divisant par leurs écarts types ? respectifs. Le but affiché de cette opération est de créer des variables sans unités pour pouvoir les comparer entre elles, en particulier quand l'estimateur est basé sur la vraisemblance pénalisée ou une distribution a priori. Nous attendons prouver que cette vision est trop simpliste. Nous proposons de plutôt considérer un élément plus essentiel de la matrice de régression en standardisant les variables explicatives à partir des éléments diagonaux de la matrice de covariance ? de l'estimateur du maximum de vraisemblance. Nous illustrons les différences entre la standardisation ? et la standarisation ? avec des estimateurs et des données variés.</TD></TR></table><P align='left' style='color:#993300'><B><u>10.2307/27919609</u></B></P><BR><P align='left' style='color:#CC66FF'><B><u>Current Issuemap</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>This work proposes a new Shewhart-type control chart of the Weibull percentile (i.e. the reliable life) as a practical example of a product attained following the Data Technology (DT) approach. DT is briefly introduced as a new discipline defined apart from Information Technology (IT). Following this approach, some specific Bayes estimators are selected from literature and then used to build the above new chart. These estimators allow to improve the control making use of any available kind of data (statistical and non-statistical). The operative steps of DT approach are fully explained. The results are illustrated by means of a real applicative example.</TD></TR></table><P align='left' style='color:#FF9900'><B><u>Most Eligible Issuemap Info</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>This work proposes a new Shewhart-type control chart of the Weibull percentile (i.e. the reliable life) as a practical example of a product attained following the Data Technology (DT) approach. DT is briefly introduced as a new discipline defined apart from Information Technology (IT). Following this approach, some specific Bayes estimators are selected from literature and then used to build the above new chart. These estimators allow to improve the control making use of any available kind of data (statistical and non-statistical). The operative steps of DT approach are fully explained. The results are illustrated by means of a real applicative example.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>2</TD><TD width='5%' bordercolor='blue'>Ce travail illustre une nouvelle carte de contrôle de un percentile (la vie fiable) avec la distribution de Weibull, par un exemple pratique de un nouveau produit à la suite de l'approche nommée Technologie de les Données (DT). La DT est présentée comme une nouvelle discipline à part de la Technologie de l'Information (IT). A' la suite de ce approche, le travail démontre la choix et l'utilisation des estimateurs bayesian connues sous le nom "Estimateurs Bayesian Pratiques", qui sont appliquées pour développer la nouvelle carte. Ces estimateurs donne plus d'efficacité au processus d'évaluation statistique sans abîmer aucunes informations et aidant l'amélioration de l'estimation et le contrôle de la fiabilité. Un exemple concret montre les résultats de l'application pratique de la Technologie de les Données (DT).</TD></TR></table>
				
		</td>
		</tr>
	</table>		
</body>
</html>
