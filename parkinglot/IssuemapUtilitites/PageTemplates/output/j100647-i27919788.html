<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
	<TITLE>$titleTag</TITLE>
<STYLE type="text/css">
   subtoolbar {border-top-width: 0px; border-top-style: none;}


   
 </STYLE>	
</head>
<body>
	
	<table width="940" height="496" border="0" align="center" cellpadding="0" cellspacing="0">
		<tr>
			<td height="489" align="right" valign="top"><SPAN><BR><INPUT type="BUTTON" name="BUTTON" value="< BACK" onclick="javascript: history.go(-1)" /></SPAN><BR>
			<table width="100%">
				<tr valign="top" align="left" style="color:#0066FF">
					<th width="33%">j100647</th>
					<th width="33%">i27919788</th>
					<th width="33%"><A HREF='http://phoenix.jstor.org/Phoenix/toc/secure/issue.html?workType=mod&journalId=10.2307/j100647&issueId=10.2307_i27919788' target='_blank'>PHX Link</A></th>
				</tr>
			</table>
			<P align='left' style='color:#993300'><B><u>10.2307/27919792</u></B></P><BR><P align='left' style='color:#CC66FF'><B><u>Current Issuemap</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>This paper discusses the challenges in assessing the effects of mixed modes on measurement. We briefly review theories about why different modes of data collection can lead to differences in survey responses and statistical methods typically used to assess mode effects. We then discuss the challenges, including which mode effects are identified, how to test for mode effects, and whether these would affect substantive conclusions. The issues raised are illustrated with examples from the European Social Survey, which is conducting a programme of experimental research to inform decisions about whether to use mixed modes of data collection. The paper concludes with general implications for mixed modes research.</TD></TR></table><P align='left' style='color:#FF9900'><B><u>Most Eligible Issuemap Info</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>This paper discusses the challenges in assessing the effects of mixed modes on measurement. We briefly review theories about why different modes of data collection can lead to differences in survey responses and statistical methods typically used to assess mode effects. We then discuss the challenges, including which mode effects are identified, how to test for mode effects, and whether these would affect substantive conclusions. The issues raised are illustrated with examples from the European Social Survey, which is conducting a programme of experimental research to inform decisions about whether to use mixed modes of data collection. The paper concludes with general implications for mixed modes research.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>2</TD><TD width='5%' bordercolor='blue'>Cet article passe en revue les défis relatifs à l'évaluation de l'effet sur l'erreur mesure, de l'utilisation de plusieurs modes de collecte des données. Nous débutons par une revue succincte des théories sur la façon dont différents modes peuvent produire des réponses divergentes pour ensuite traiter des méthodes statistiques généralement utilisées afin d'évaluer l'effet du mode de collecte. Ensuite nous discutons les défis, y compris quels effets du mode de collecte sur l'erreur mesure sont identifiés, comment tester l'effet du mode et si les différences entre modes impactent sur les conclusions de fond. Les considérations sont illustrées avec des examples de l'Enquête Sociale Européenne où un programme de recherche est présentement en cours afin d'évaluer la faisabilité d'utiliser plusieurs modes de collecte des données. L'article se termine par des recommandations générales pertinentes pour la poursuite de la recherche sur les modes de collecte multiples.</TD></TR></table><P align='left' style='color:#993300'><B><u>10.2307/27919793</u></B></P><BR><P align='left' style='color:#CC66FF'><B><u>Current Issuemap</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>Calibration estimation, where the sampling weights are adjusted to make certain estimators match known population totals, is commonly used in survey sampling. The generalized regression estimator is an example of a calibration estimator. Given the functional form of the calibration adjustment term, we establish the asymptotic equivalence between the functional-form calibration estimator and an instrumental variable calibration estimator where the instrumental variable is directly determined from the functional form in the calibration equation. Variance estimation based on linearization is discussed and applied to some recently proposed calibration estimators. The results are extended to the estimator that is a solution to the calibrated estimating equation. Results from a limited simulation study are presented.</TD></TR></table><P align='left' style='color:#FF9900'><B><u>Most Eligible Issuemap Info</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>Calibration estimation, where the sampling weights are adjusted to make certain estimators match known population totals, is commonly used in survey sampling. The generalized regression estimator is an example of a calibration estimator. Given the functional form of the calibration adjustment term, we establish the asymptotic equivalence between the functional-form calibration estimator and an instrumental variable calibration estimator where the instrumental variable is directly determined from the functional form in the calibration equation. Variance estimation based on linearization is discussed and applied to some recently proposed calibration estimators. The results are extended to the estimator that is a solution to the calibrated estimating equation. Results from a limited simulation study are presented.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>2</TD><TD width='5%' bordercolor='blue'>L'estimation par calage, pour laquelle les poids de sondage sont ajustés de manière à ce que certains estimateurs coïncident avec destotaux connus dans la population, est fréquemment utilisée en échantillonnage. L'estimateur par la régression généralisée est un exemple d'un estimateur de calage. Dans le cas où les facteurs d'ajustement sont exprimés selon une forme fonctionnelle, nous établissons l'équivalence asymptotique entre l'estimateur de calage ave celui avec variable instrumentale, où la variable instrumentale est directement déterminée à partir de la forme fonctionnelle dans l'équation de calage. L'estimation de la variance par linéarisation est traitée et appliquée à certains estimateurs de calage proposés récemment. Les résultats sont généralisés à l'estimateur solution de l'équation estimante calée. Les résultats d'une étude par simulation limitée sont présentés.</TD></TR></table><P align='left' style='color:#993300'><B><u>10.2307/27919794</u></B></P><BR><P align='left' style='color:#CC66FF'><B><u>Current Issuemap</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>Hot deck imputation is a method for handling missing data in which each missing value is replaced with an observed response from a “similar” unit. Despite being used extensively in practice, the theory is not as well developed as that of other imputation methods. We have found that no consensus exists as to the best way to apply the hot deck and obtain inferences from the completed data set. Here we review different forms of the hot deck and existing research on its statistical properties. We describe applications of the hot deck currently in use, including the U.S. Census Bureau's hot deck for the Current Population Survey (CPS). We also provide an extended example of variations of the hot deck applied to the third National Health and Nutrition Examination Survey (NHANES III). Some potential areas for future research are highlighted.</TD></TR></table><P align='left' style='color:#FF9900'><B><u>Most Eligible Issuemap Info</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>Hot deck imputation is a method for handling missing data in which each missing value is replaced with an observed response from a “similar” unit. Despite being used extensively in practice, the theory is not as well developed as that of other imputation methods. We have found that no consensus exists as to the best way to apply the hot deck and obtain inferences from the completed data set. Here we review different forms of the hot deck and existing research on its statistical properties. We describe applications of the hot deck currently in use, including the U.S. Census Bureau's hot deck for the Current Population Survey (CPS). We also provide an extended example of variations of the hot deck applied to the third National Health and Nutrition Examination Survey (NHANES III). Some potential areas for future research are highlighted.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>2</TD><TD width='5%' bordercolor='blue'>L'imputation hot deck est une méthode de gestion des données manquantes dans laquelle chaque valeur manquante est remplacée par une réponse observée à partir d'une unité “similaire.” Bien qu'elle soit largement utilisée en pratique, sa théorie n'est pas aussi développée que celle des autres méthodes d'imputation. Nous avons constaté qu'il n'existe aucun consensus quant à la meilleure faon d'appliquer les hot deck et obtenir des inférences à partir de la série de données complète. Ici, nous passons en revue les différentes formes de hot deck et les recherches existantes sur ses propriétés statistiques. Nous décrivons les applications du hot deck actuellement utilisées, y compris le hot deck du Bureau US du recensement pour la Current Population Survey (CPS). Nous proposons aussi des exemples nombreux de variations du hot deck à la troisième National Health and Nutrition Examination Survey (NHANES III). Certains domaines possibles de recherches futures sont mises en évidence.</TD></TR></table><P align='left' style='color:#993300'><B><u>10.2307/27919795</u></B></P><BR><P align='left' style='color:#CC66FF'><B><u>Current Issuemap</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>An ?-permanental random field is briefly speaking a model for a collection of non-negative integer valued random variables with positive associations. Though such models possess many appealing probabilistic properties, many statisticians seem unaware of ?-permanental random fields and their potential applications. The purpose of this paper is to summarize useful probabilistic results, study stochastic constructions and simulation techniques, and discuss some examples of ?-permanental random fields. This should provide a useful basis for discussing the statistical aspects in future work.</TD></TR></table><P align='left' style='color:#FF9900'><B><u>Most Eligible Issuemap Info</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>An ?-permanental random field is briefly speaking a model for a collection of non-negative integer valued random variables with positive associations. Though such models possess many appealing probabilistic properties, many statisticians seem unaware of ?-permanental random fields and their potential applications. The purpose of this paper is to summarize useful probabilistic results, study stochastic constructions and simulation techniques, and discuss some examples of ?-permanental random fields. This should provide a useful basis for discussing the statistical aspects in future work.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>2</TD><TD width='5%' bordercolor='blue'>Un champ aléatoire ?-permanental est la modélisation d'une collection de variables aléatoires entières non négatives positivement associées. Malgré leurs attrayantes propriétés probabilistes et leurs nombreuses applications potentielles, ces modèles restent ignorés de la majorité des statisticiens. Le but de cet article est de fournir les résultats probabilistes utiles, d'étudier les constructions stochastiques et les techniques de simulation, ainsi que de discuter quelques exemples, de façon à fournir une base utile à une discussion future des aspects statistiques des champs permanentaux.</TD></TR></table><P align='left' style='color:#993300'><B><u>10.2307/27919797</u></B></P><BR><P align='left' style='color:#CC66FF'><B><u>Current Issuemap</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>According to the law of likelihood, statistical evidence is represented by likelihood functions and its strength measured by likelihood ratios. This point of view has led to a likelihood paradigm for interpreting statistical evidence, which carefully distinguishes evidence about a parameter from error probabilities and personal belief. Like other paradigms of statistics, the likelihood paradigm faces challenges when data are observed incompletely, due to non-response or censoring, for instance. Standard methods to generate likelihood functions in such circumstances generally require assumptions about the mechanism that governs the incomplete observation of data, assumptions that usually rely on external information and cannot be validated with the observed data. Without reliable external information, the use of untestable assumptions driven by convenience could potentially compromise the interpretability of the resulting likelihood as an objective representation of the observed evidence. This paper proposes a profile likelihood approach for representing and interpreting statistical evidence with incomplete data without imposing untestable assumptions. The proposed approach is based on partial identification and is illustrated with several statistical problems involving missing data or censored data. Numerical examples based on real data are presented to demonstrate the feasibility of the approach.</TD></TR></table><P align='left' style='color:#FF9900'><B><u>Most Eligible Issuemap Info</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>According to the law of likelihood, statistical evidence is represented by likelihood functions and its strength measured by likelihood ratios. This point of view has led to a likelihood paradigm for interpreting statistical evidence, which carefully distinguishes evidence about a parameter from error probabilities and personal belief. Like other paradigms of statistics, the likelihood paradigm faces challenges when data are observed incompletely, due to non-response or censoring, for instance. Standard methods to generate likelihood functions in such circumstances generally require assumptions about the mechanism that governs the incomplete observation of data, assumptions that usually rely on external information and cannot be validated with the observed data. Without reliable external information, the use of untestable assumptions driven by convenience could potentially compromise the interpretability of the resulting likelihood as an objective representation of the observed evidence. This paper proposes a profile likelihood approach for representing and interpreting statistical evidence with incomplete data without imposing untestable assumptions. The proposed approach is based on partial identification and is illustrated with several statistical problems involving missing data or censored data. Numerical examples based on real data are presented to demonstrate the feasibility of the approach.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>2</TD><TD width='5%' bordercolor='blue'>Selon la loi des vraisemblances, les preuves statistiques sont représentées par des fonctions de vraisemblance et leur solidité est mesurée par des rapports de vraisemblance. Ce point de vue a conduit à un paradigme de la vraisemblance destiné à interpréter les preuves statistiques et qui fait soigneusement la distinction entre les preuves d'un paramètre et les probabilités d'erreur, ainsi que les croyances personnelles. Comme c'est le cas pour d'autres paradigmes de statistiques, le paradigme de la vraisemblance est confronté à des défis en cas d'observation incompléte de données en raison de l'absence de réponses ou de censure, par exemple. Les méthodes classiques destinées à générer des fonctions de vraisemblance dans de telles circonstances, exigent généralement des suppositions sur le mécanisme régissant l'observation incomplète de données, suppositions qui reposent habituellement sur des informations extérieures ne pouvant pas être validées par les données observées. Sans informations externes fiables, l'usage de suppositions non vérifiables entraînées par la commodité pourrait potentiellement compromettre l'interprétation de la vraisemblance qui en résulte, en tant que représentation objective des preuves observées. Cet article propose une approche de vraisemblance de profil pour représenter et interpréter des preuves statistiques avec des données incomplètes, sans imposer de suppositions non vérifiables. L'approche proposée repose sur une identification partielle et elle est illustrée au moyen de plusieurs problèmes statistiques mettant en jeu des données manquantes ou censurées. Des exemples numériques reposant sur des données réelles sont présentés, afin de démontrer la faisabilité de l'approche.</TD></TR></table><P align='left' style='color:#993300'><B><u>10.2307/27919796</u></B></P><BR><P align='left' style='color:#CC66FF'><B><u>Current Issuemap</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>Besides a brief historical outline of the Gini index decomposition proposals, we compare, in a subgroups framework, the decompositions of the Gini index and of the uniformity and inequality indexes recently proposed by Zenga. The two decompositions follow a similar scheme: in both cases the overall index can be at first expressed as a weighted average of convenient “cross” measures, with their own interpretation, and afterwards it is decomposed into a within and a between term by merely distinguishing measures evaluated within the same subgroup from the ones regarding different subgroups. This procedure does not depend on an a priori definition of the within or the between term and allows their contribution to be naturally evaluated preserving the structure of the index itself. In the last section the two decompositions are applied to the Italian net household wealth provided by the 2006 central Bank of Italy sample survey on household budgets.</TD></TR></table><P align='left' style='color:#FF9900'><B><u>Most Eligible Issuemap Info</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>Besides a brief historical outline of the Gini index decomposition proposals, we compare, in a subgroups framework, the decompositions of the Gini index and of the uniformity and inequality indexes recently proposed by Zenga. The two decompositions follow a similar scheme: in both cases the overall index can be at first expressed as a weighted average of convenient “cross” measures, with their own interpretation, and afterwards it is decomposed into a within and a between term by merely distinguishing measures evaluated within the same subgroup from the ones regarding different subgroups. This procedure does not depend on an a priori definition of the within or the between term and allows their contribution to be naturally evaluated preserving the structure of the index itself. In the last section the two decompositions are applied to the Italian net household wealth provided by the 2006 central Bank of Italy sample survey on household budgets.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>2</TD><TD width='5%' bordercolor='blue'>Après une concise revue de la littérature sur la décomposition de l'indice de concentration de Gini en sous-groupes, cette article confronte les décompositions de la mesure de Gini avec celles d'uniformité et d'inégalité récemment proposées par Zenga. Les deux décompositions suivent un approche similaire: l'indice global d'inégalité s'écrit sous la forme d'une moyenne pondérée des adéquates mesures croisés, de facile interprétation. Après il est décomposé en deux composantes: celle entre les groupes et celle dans les groupes, simplement avec la séparation des comparaisons à l'intérieur de chaque groupe de celles entre les différents groupes. Cette procédure est indépendant d'une définition a priori des éléments entre les groupes et dans les groupes et elle est capable de calculer leurs contributions à l'inégalité totale et de préserver la structure de l'indice. Dans le dernier paragraphe les deux décompositions sont appliquées aux données sur la richesse des ménages italien récoltées par le sondage de 2006 de la Banque Centrale d'Italie.</TD></TR></table><P align='left' style='color:#993300'><B><u>10.2307/27919798</u></B></P><BR><P align='left' style='color:#CC66FF'><B><u>Current Issuemap</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>Maximum likelihood estimates are obtained for long data sets of bivariate financial returns using mixing representation of the bivariate (skew) Variance Gamma (VG) and two (skew) t distributions. By analysing simulated and real data, issues such as asymptotic lower tail dependence and competitiveness of the three models are illustrated. A brief review of the properties of the models is included. The present paper is a companion to papers in this journal by Demarta & McNeil and Finlay & Seneta.</TD></TR></table><P align='left' style='color:#FF9900'><B><u>Most Eligible Issuemap Info</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>Maximum likelihood estimates are obtained for long data sets of bivariate financial returns using mixing representation of the bivariate (skew) Variance Gamma (VG) and two (skew) t distributions. By analysing simulated and real data, issues such as asymptotic lower tail dependence and competitiveness of the three models are illustrated. A brief review of the properties of the models is included. The present paper is a companion to papers in this journal by Demarta & McNeil and Finlay & Seneta.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>2</TD><TD width='5%' bordercolor='blue'>Des estimateurs maximum de vraisemblance sont obtenus pour de longues séries bivariées de rendements financiers modélisées à partir d'un mélange (asymétrique) de type Variance-Gamma et de deux mélanges (asymétriques) de type Student. L'analyse de données simulées et réelles permet d'illustrer quelques-uns des aspects asymptotiques de ces trois modèles, tels que les dépendances asymptotiques des extrêmes dans la queue gauche, et leurs performances. Un bref compte-rendu des propriétiés de ces modèles est également inclus. Le présent travail accompagne et complète les articles de Demarta et McNeil (2005) et de Finlay et Seneta (2008) parus dans la même revue.</TD></TR></table>
				
		</td>
		</tr>
	</table>		
</body>
</html>
