<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
	<TITLE>$titleTag</TITLE>
<STYLE type="text/css">
   subtoolbar {border-top-width: 0px; border-top-style: none;}


   
 </STYLE>	
</head>
<body>
	
	<table width="940" height="496" border="0" align="center" cellpadding="0" cellspacing="0">
		<tr>
			<td height="489" align="right" valign="top"><SPAN><BR><INPUT type="BUTTON" name="BUTTON" value="< BACK" onclick="javascript: history.go(-1)" /></SPAN><BR>
			<table width="100%">
				<tr valign="top" align="left" style="color:#0066FF">
					<th width="33%">j100647</th>
					<th width="33%">i27919718</th>
					<th width="33%"><A HREF='http://phoenix.jstor.org/Phoenix/toc/secure/issue.html?workType=mod&journalId=10.2307/j100647&issueId=10.2307_i27919718' target='_blank'>PHX Link</A></th>
				</tr>
			</table>
			<P align='left' style='color:#993300'><B><u>10.2307/27919725</u></B></P><BR><P align='left' style='color:#CC66FF'><B><u>Current Issuemap</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>We examine the difference between Bayesian and frequentist statistics in making statements about the relationship between observable values. We show how standard models under both paradigms can be based on an assumption of exchangeability and we derive useful covariance and correlation results for values from an exchangeable sequence. We find that such values are never negatively correlated, and are generally positively correlated under the models used in Bayesian statistics. We discuss the significance of this result as well as a phenomenon which often follows from the differing methodologies and practical applications of these paradigms ? a phenomenon we call Bayes' effect.</TD></TR></table><P align='left' style='color:#FF9900'><B><u>Most Eligible Issuemap Info</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>We examine the difference between Bayesian and frequentist statistics in making statements about the relationship between observable values. We show how standard models under both paradigms can be based on an assumption of exchangeability and we derive useful covariance and correlation results for values from an exchangeable sequence. We find that such values are never negatively correlated, and are generally positively correlated under the models used in Bayesian statistics. We discuss the significance of this result as well as a phenomenon which often follows from the differing methodologies and practical applications of these paradigms ? a phenomenon we call Bayes' effect.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>2</TD><TD width='5%' bordercolor='blue'>Nous examinons la différence entre les statistiques Bayesiennes et fréquentistes dans des propositions sur la relation entre valeurs observées. Nous démontrons comment les modèles normaux dans les deux cas peuvent être basés sur la supposition d'échangeabilité, et nous obtenons quelques résultats utiles sur la covariance et la corrélation pour des valeurs dans une suite échangeable. Ces valeurs ne sont jamais corrélées négativement, et sont en général corrélées positivement dans les modèles Bayesiens. Nous discutons la signification de ce résultat, ainsi que celui du phénomène qui s'ensuit lorsqu'on emploie ces deux méthodologies, un phénomène que nous appelons l'effet de Bayes.</TD></TR></table><P align='left' style='color:#993300'><B><u>10.2307/27919726</u></B></P><BR><P align='left' style='color:#CC66FF'><B><u>Current Issuemap</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>Using an Edgeworth expansion to speed up the asymptotics, we develop one-sided coverage intervals for a proportion based on a stratified simple random sample. To this end, we assume the values of the population units are generated from independent random variables with a common mean within each stratum. These stratum means, in turn, may either be free to vary or are assumed to be equal. The more general assumption is equivalent to a model-free randomization-based framework when finite population correction is ignored. Unlike when an Edgeworth expansion is used to construct one-sided intervals under simple random sampling, it is necessary to estimate the variance of the estimator for the population proportion when the stratum means are allowed to differ. As a result, there may be accuracy gains from replacing the normal z-score in the Edgeworth expansion with a t-score.</TD></TR></table><P align='left' style='color:#FF9900'><B><u>Most Eligible Issuemap Info</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>Using an Edgeworth expansion to speed up the asymptotics, we develop one-sided coverage intervals for a proportion based on a stratified simple random sample. To this end, we assume the values of the population units are generated from independent random variables with a common mean within each stratum. These stratum means, in turn, may either be free to vary or are assumed to be equal. The more general assumption is equivalent to a model-free randomization-based framework when finite population correction is ignored. Unlike when an Edgeworth expansion is used to construct one-sided intervals under simple random sampling, it is necessary to estimate the variance of the estimator for the population proportion when the stratum means are allowed to differ. As a result, there may be accuracy gains from replacing the normal z-score in the Edgeworth expansion with a t-score.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>2</TD><TD width='5%' bordercolor='blue'>Nous développons des intervalles de confiance unilatéraux pour une proportion, lorsqu'un échantillon aléatoire simple est tiré d'une population, en utilisant un développement en séries de Edgeworth pour accélérer la convergence. Pour obtenir ces intervalles, nous supposons que les valeurs des unités de la population sont générées à partir de variables aléatoires indépendantes avec la même moyenne à l'intérieur de chaque strate. Ces moyennes de strate peuvent, à leur tour, soit être libres de varier ou être supposées constantes. L'hypothèse la plus générale est équivalente à utiliser un cadre de travail basé sur le plan de sondage (ou “randomization-based”), qui ne nécessite donc pas d'hypothèses au sujet d'un modèle, et où l'on ignore la correction pour populations finies. Contrairement au cas dans lequel un développement en séries de Edgeworth est utilisé pour construire des intervalles unilatéraux sous l'échantillonnage aléatoire simple, il est nécessaire de permettre aux moyennes des strates d'être différentes les unes des autres lorsqu'on estime la variance de l'estimateur de la proportion dans la population. Par conséquent, il peut y avoir des gains de précision lorsqu'on remplace le score z normal dans la série de Edgeworth par un score t.</TD></TR></table><P align='left' style='color:#993300'><B><u>10.2307/27919723</u></B></P><BR><P align='left' style='color:#CC66FF'><B><u>Current Issuemap</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>A method, which we believe is simpler and more transparent than the one due to McCullagh (1984), is described for obtaining the cumulants of a scalar multivariate stochastic Taylor expansion. Its generalisation is also suggested. An important feature, previously not reported, is that the expansion of every cumulant of order ? 2 is made up of separate subseries.</TD></TR></table><P align='left' style='color:#FF9900'><B><u>Most Eligible Issuemap Info</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>A method, which we believe is simpler and more transparent than the one due to McCullagh (1984), is described for obtaining the cumulants of a scalar multivariate stochastic Taylor expansion. Its generalisation is also suggested. An important feature, previously not reported, is that the expansion of every cumulant of order ? 2 is made up of separate subseries.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>2</TD><TD width='5%' bordercolor='blue'>In order to handle certain frequently occurring sums over permutations of members of compound index sets, we introduce a new notation [m], where m is a positive integer.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>3</TD><TD width='5%' bordercolor='blue'>Une méthode plus simple et plus transparente que celle de McCullagh (1984), est décrite pour obtenir les cumulants d'une expansion de Taylor scalaire, stochastique à plusieurs variables. Sa généralisation est aussi suggérée. Une caractéristique importante pas signalée auparavant est que l'expansion de chaque cumulant de l'ordre ? 2 est composée de sous-séries séparées.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>4</TD><TD width='5%' bordercolor='blue'>Pour pouvoir traiter certaines sommes qui apparaissent fréquemment sur des permutations des membres des ensembles d'indices composés, nous introduisons une nouvelle notation [m], où m est un entier relatif appositif.</TD></TR></table><P align='left' style='color:#993300'><B><u>10.2307/27919724</u></B></P><BR><P align='left' style='color:#CC66FF'><B><u>Current Issuemap</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>Monitoring small area contrasts in life expectancy is important for health policy purposes but subject to difficulties under conventional life table analysis. Additionally, the implicit model underlying conventional life table analysis involves a highly parametrized fixed effect approach. An alternative strategy proposed here involves an explicit model based on random effects for both small areas and age groups. The area effects are assumed to be spatially correlated, reflecting unknown mortality risk factors that are themselves typically spatially correlated. Often mortality observations are disaggregated by demographic category as well as by age and area, e.g. by gender or ethnic group, and multivariate area and age random effects will be used to pool over such groups. A case study considers variations in life expectancy in 1 118 small areas (known as wards) in Eastern England over a five-year period 1999–2003. The case study deaths data are classified by gender, age, and area, and a bivariate model for area and age effects is therefore applied. The interrelationship between the random area effects and two major influences on small area life expectancy is demonstrated in the study, these being area socio-economic status (or deprivation) and the location of nursing and residential homes for frail elderly.</TD></TR></table><P align='left' style='color:#FF9900'><B><u>Most Eligible Issuemap Info</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>Monitoring small area contrasts in life expectancy is important for health policy purposes but subject to difficulties under conventional life table analysis. Additionally, the implicit model underlying conventional life table analysis involves a highly parametrized fixed effect approach. An alternative strategy proposed here involves an explicit model based on random effects for both small areas and age groups. The area effects are assumed to be spatially correlated, reflecting unknown mortality risk factors that are themselves typically spatially correlated. Often mortality observations are disaggregated by demographic category as well as by age and area, e.g. by gender or ethnic group, and multivariate area and age random effects will be used to pool over such groups. A case study considers variations in life expectancy in 1 118 small areas (known as wards) in Eastern England over a five-year period 1999–2003. The case study deaths data are classified by gender, age, and area, and a bivariate model for area and age effects is therefore applied. The interrelationship between the random area effects and two major influences on small area life expectancy is demonstrated in the study, these being area socio-economic status (or deprivation) and the location of nursing and residential homes for frail elderly.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>2</TD><TD width='5%' bordercolor='blue'>Le suivi des contrastes d'espérance de vie entre petites régions est important pour les politiques de santé mais l'analyse en est difficile avec les tables de vie conventionnelles. De plus le modèle implicite qui sous-tend l'analyse conventionnelle des tables de vie inclut une approche d'effet fixe fortement paramétrée. On propose ici une stratégie alternative qui comprend un modèle explicite basé sur des effets aléatoires pour des petites zones ainsi que des groupes d'âge. Les effets de zone sont supposés être corrélés spatialement, reflétant des facteurs de risque de mortalité inconnus, eux-mêmes corrélés spatialement. Les observations de mortalité sont souvent désagrégées par catégorie démographique de même que par âge et région, par sexe ou groupe ethnique, et les effets aléatoires multivariés de région et d'âge seront utilisés pour mettre en commun de tels groupes. Une étude de cas considère les variations d'espérance de vie dans 1118 petites zones (connues comme unités/circonscriptions) en Angleterre orientale sur une période de cinq ans 1999–2003. Les données de mortalité de l'étude de cas sont classées par sexe, âge et zone, et un modèle bivarié pour les effets de zone et d'âge est appliqué. L'interrelation entre les effets aléatoires de zone et deux influences majeures sur l'espérance de vie dans une petite zone sont démontrée dans l'étude: ce sont le statut socioéconomique de la zone et la localization des soins (infirmières) et des résidences pour personnes âgées en situation précaire.</TD></TR></table><P align='left' style='color:#993300'><B><u>10.2307/27919721</u></B></P><BR><P align='left' style='color:#CC66FF'><B><u>Current Issuemap</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>In data integration contexts, two statistical agencies seek to merge their separate databases into one file. The agencies also may seek to disseminate data to the public based on the integrated file. These goals may be complicated by the agencies' need to protect the confidentiality of database subjects, which could be at risk during the integration or dissemination stage. This article proposes several approaches based on multiple imputation for disclosure limitation, usually called synthetic data, that could be used to facilitate data integration and dissemination while protecting data confidentiality. It reviews existing methods for obtaining inferences from synthetic data and points out where new methods are needed to implement the data integration proposals.</TD></TR></table><P align='left' style='color:#FF9900'><B><u>Most Eligible Issuemap Info</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>In data integration contexts, two statistical agencies seek to merge their separate databases into one file. The agencies also may seek to disseminate data to the public based on the integrated file. These goals may be complicated by the agencies' need to protect the confidentiality of database subjects, which could be at risk during the integration or dissemination stage. This article proposes several approaches based on multiple imputation for disclosure limitation, usually called synthetic data, that could be used to facilitate data integration and dissemination while protecting data confidentiality. It reviews existing methods for obtaining inferences from synthetic data and points out where new methods are needed to implement the data integration proposals.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>2</TD><TD width='5%' bordercolor='blue'>Dans les contextes d'intégration de données, deux agences statistiques cherchent à fusionner leurs bases de données séparées en un fichier. Les agences peuvent aussi chercher à diffuser au public les données issues du fichier intégré. Ces objectifs peuvent être compliqués par le besoin de protéger la confidentialité des objets de la base de données, qui pourrait être menacé pendant la phase d'intégration et de diffusion. Cet article propose plusieurs approches basées sur l'imputation multiple pour limiter la divulgation, qu'on appelle habituellement données synthétiques, qui pourraient être utilis©es pour faciliter l'int©gration et la diffusion des donn©es tout en prot©geant leur confidentialit©. Il passe en revue les méthodes existantes pour obtenir des inférences à partir de données synthétiques et montre les cas où l'on a besoin de nouvelles méthodes pour mettre en œuvre les propositions d'intégration de données.</TD></TR></table><P align='left' style='color:#993300'><B><u>10.2307/27919722</u></B></P><BR><P align='left' style='color:#CC66FF'><B><u>Current Issuemap</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>For continuous key variables, a measure of the individual risk of disclosure is proposed. This risk measure, the local outlier factor, estimates the density around a unit. A selective masking method based on the nearest-neighbour principle and microaggregation is also introduced. Some results of an application to the Italian sample of the Community Innovation Survey are presented.</TD></TR></table><P align='left' style='color:#FF9900'><B><u>Most Eligible Issuemap Info</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>For continuous key variables, a measure of the individual risk of disclosure is proposed. This risk measure, the local outlier factor, estimates the density around a unit. A selective masking method based on the nearest-neighbour principle and microaggregation is also introduced. Some results of an application to the Italian sample of the Community Innovation Survey are presented.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>2</TD><TD width='5%' bordercolor='blue'>Pour les clefs d'identification continues, une mesure du risque individuel de réidentification est proposée. Cette mesure, le facteur de donnée aberrante locale, estime la densité autour d'une unité. Une méthode de protection sélective, basée sur le principe des plus proches unités et sur la micro-agrégation, est également introduite. Des résultats de l'application à l'anonymisation de fichier de données individuelles provenant de l'échantillon italien de l'enquête « Community Innovation » sont presentés.</TD></TR></table><P align='left' style='color:#993300'><B><u>10.2307/27919720</u></B></P><BR><P align='left' style='color:#CC66FF'><B><u>Current Issuemap</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>Traditional and choice-based conjoint analyses (CAs) have used full or fractional factorial designs to generate concept profile descriptions. However, these designs confound two factors when costs are associated with attributes: first is the total cost of the concept profile, and second is the allocation of costs among the attributes. Both factors may influence consumers' preferences. So far, these issues have not been separated in the CA literature. The present paper shows how mixture?amount designs used in industrial experiments may be used to separate amount and mixture effects in traditional CA. The extension to choice-based CA using balance incomplete block (BIB) designs is also given.</TD></TR></table><P align='left' style='color:#FF9900'><B><u>Most Eligible Issuemap Info</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>Traditional and choice-based conjoint analyses (CAs) have used full or fractional factorial designs to generate concept profile descriptions. However, these designs confound two factors when costs are associated with attributes: first is the total cost of the concept profile, and second is the allocation of costs among the attributes. Both factors may influence consumers' preferences. So far, these issues have not been separated in the CA literature. The present paper shows how mixture?amount designs used in industrial experiments may be used to separate amount and mixture effects in traditional CA. The extension to choice-based CA using balance incomplete block (BIB) designs is also given.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>2</TD><TD width='5%' bordercolor='blue'>L'analyse conjointe, classique ou basée sur les choix (CBC), a utilisé des plans factoriels complets ou fractionnés pour générer des descriptions de profils de concepts. Toutefois, ces plans confondent deux facteurs lorsque les coûts sont associés aux attributs: le premier est le coût total du profil de concept et le second est la répartition des coûts entre les attributs. Ces deux facteurs peuvent influencer les préférences des consommateurs. Jusqu' à présent, la littérature sur l'analyse conjointe n'a pas traité séparément ces problèmes. Cet article montre comment des plans de mélange mixture?amount utilisés dans des expériences en milieu industriel peuvent être utilisés pour séparer des effets de type montant et des effets de type mélange dans une analyse conjointe traditionnelle. Une extension à l'analyse conjointe CBC fondée sur des plans en blocs incomplets équilibrés (BIB) est également proposée.</TD></TR></table><P align='left' style='color:#993300'><B><u>10.2307/27919727</u></B></P><BR><P align='left' style='color:#CC66FF'><B><u>Current Issuemap</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>The Rule of Three (R3) states that 3/n is an approximate 95% upper limit for the binomial parameter, when there are no events in n trials. This rule is based on the one-sided Clopper–Pearson exact limit, but it is shown that none of the other popular frequentist methods lead to it. It can be seen as a special case of a Bayesian R3, but it is shown that among common choices for a non-informative prior, only the Bayes–Laplace and Zellner priors conform with it. R3 has also incorrectly been extended to 3 being a "reasonable" upper limit for the number of events in a future experiment of the same (large) size, when, instead, it applies to the binomial mean. In Bayesian estimation, such a limit should follow from the posterior predictive distribution. This method seems to give more natural results than—though when based on the Bayes–Laplace prior technically converges with—the method of prediction limits, which indicates between 87.5% and 93.75% confidence for this extended R3. These results shed light on R3 in general, suggest an extended Rule of Four for a number of events, provide a unique comparison of Bayesian and frequentist limits, and support the choice of the Bayes–Laplace prior among non-informative contenders.</TD></TR></table><P align='left' style='color:#FF9900'><B><u>Most Eligible Issuemap Info</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>The Rule of Three (R3) states that 3/n is an approximate 95% upper limit for the binomial parameter, when there are no events in n trials. This rule is based on the one-sided Clopper–Pearson exact limit, but it is shown that none of the other popular frequentist methods lead to it. It can be seen as a special case of a Bayesian R3, but it is shown that among common choices for a non-informative prior, only the Bayes–Laplace and Zellner priors conform with it. R3 has also incorrectly been extended to 3 being a "reasonable" upper limit for the number of events in a future experiment of the same (large) size, when, instead, it applies to the binomial mean. In Bayesian estimation, such a limit should follow from the posterior predictive distribution. This method seems to give more natural results than—though when based on the Bayes–Laplace prior technically converges with—the method of prediction limits, which indicates between 87.5% and 93.75% confidence for this extended R3. These results shed light on R3 in general, suggest an extended Rule of Four for a number of events, provide a unique comparison of Bayesian and frequentist limits, and support the choice of the Bayes–Laplace prior among non-informative contenders.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>2</TD><TD width='5%' bordercolor='blue'>La Règle de Trois établit que 3/n est une limite supérieure approchée de 95% pour le paramètre de la loi binomiale quand aucun événement n'est observé en n essais. Cette règle est basée sur la limite unilatérale de Clopper-Pearson, mais il est montré qu'aucune des autres méthodes populaires fréquentistes ne s'y ramène. Elle peut être vue comme un cas particulier d'une Règle Bayésienne de Trois, mais il est montré que, parmi les choix classiques d'a priori non-informatif, seulement les a priori de Bayes-Laplace et de Zellner s'y conforment. La Règle de Trois a aussi été inexactement prolongée à 3 étant une limite supérieure 'raisonnable' pour le nombre d'événements dans une future expérience de même (grande) taille. En estimation Bayésienne, une telle limite devrait découler de la distribution prédictive a posteriori. Cette méthode semble donner des résultats plus normaux que, bien que lorsqu'elle est basée sur l'a priori de Bayes-Laplace converge techniquement avec, la méthode de limites fréquentistes de prédiction qui indiquent un niveau de confiance entre 87.5% et 93.75% pour cette Règle prolongée de Trois. Ces résultats amène un nouvel éclairage sur la Règle de Trois en général, suggérent une Règle prolongée de Quatre pour un nombre d'événements, fournissent une comparaison unique des limites Bayésiennes et fréquentistes, et renforcent le choix de l'a priori de Bayes-Laplace parmi les lois non-informatives concurrentes.</TD></TR></table><P align='left' style='color:#993300'><B><u>10.2307/27919728</u></B></P><BR><P align='left' style='color:#CC66FF'><B><u>Current Issuemap</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>The history of Quality Management, and of the role of Statistics in Quality Management, is inextricably bound to the reconstruction of Japan immediately following the Second World War, and then to developments in the United States over three decades later. Even though these periods are, in societal history, just moments ago, yet there is profound lack of agreement about what was actually done, and who should be recognized for their contributions. This paper draws on historical materials recently made publicly available in order to clarify what actually took place between 1946 and 1950, and in particular the contribution of a remarkable engineer, Homer Sarasohn.</TD></TR></table><P align='left' style='color:#FF9900'><B><u>Most Eligible Issuemap Info</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>The history of Quality Management, and of the role of Statistics in Quality Management, is inextricably bound to the reconstruction of Japan immediately following the Second World War, and then to developments in the United States over three decades later. Even though these periods are, in societal history, just moments ago, yet there is profound lack of agreement about what was actually done, and who should be recognized for their contributions. This paper draws on historical materials recently made publicly available in order to clarify what actually took place between 1946 and 1950, and in particular the contribution of a remarkable engineer, Homer Sarasohn.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>2</TD><TD width='5%' bordercolor='blue'>L'histoire de la Gestion de Qualité, et le rôle que la statistique y a joué, sont inextricablement liés à la reconstruction du Japon après la seconde guerre mondiale, puis à des développements aux Etats-Unis pendant les trente années qui ont suivi. Bien que dans l'histoire des sociétés ces périodes viennent tout juste de s'écouler, il y a cependant un profond désaccord sur ce qui a été réellement fait, et sur ceux qui pourraient être reconnus pour y avoir contribué. Cet article fait appel à du materiel historique récemment rendu disponible afin de clarifier ce qui s'est réellement passé entre 1946 et 1950, et en particulier la contribution d'un ingénieur remarquable, à savoir Homer Sarasohn.</TD></TR></table>
				
		</td>
		</tr>
	</table>		
</body>
</html>
