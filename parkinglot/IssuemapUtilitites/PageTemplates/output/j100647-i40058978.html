<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
	<TITLE>$titleTag</TITLE>
<STYLE type="text/css">
   subtoolbar {border-top-width: 0px; border-top-style: none;}


   
 </STYLE>	
</head>
<body>
	
	<table width="940" height="496" border="0" align="center" cellpadding="0" cellspacing="0">
		<tr>
			<td height="489" align="right" valign="top"><SPAN><BR><INPUT type="BUTTON" name="BUTTON" value="< BACK" onclick="javascript: history.go(-1)" /></SPAN><BR>
			<table width="100%">
				<tr valign="top" align="left" style="color:#0066FF">
					<th width="33%">j100647</th>
					<th width="33%">i40058978</th>
					<th width="33%"><A HREF='http://phoenix.jstor.org/Phoenix/toc/secure/issue.html?workType=mod&journalId=10.2307/j100647&issueId=10.2307_i40058978' target='_blank'>PHX Link</A></th>
				</tr>
			</table>
			<P align='left' style='color:#993300'><B><u>10.2307/41305059</u></B></P><BR><P align='left' style='color:#CC66FF'><B><u>Current Issuemap</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>In this paper we review statistical methods which combine hidden Markov models (HMMs) and random effects models in a longitudinal setting, leading to the class of so-called mixed HMMs. This class of models has several interesting features. It deals with the dependence of a response variable on covariates, serial dependence, and unobserved heterogeneity in an HMM framework. It exploits the properties of HMMs, such as the relatively simple dependence structure and the efficient computational procedure, and allows one to handle a variety of real-world time-dependent data. We give details of the Expectation-Maximization algorithm for computing the maximum likelihood estimates of model parameters and we illustrate the method with two real applications describing the relationship between patent counts and research and development expenditures, and between stock and market returns via the Capital Asset Pricing Model.</TD></TR></table><P align='left' style='color:#FF9900'><B><u>Most Eligible Issuemap Info</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>In this paper we review statistical methods which combine hidden Markov models (HMMs) and random effects models in a longitudinal setting, leading to the class of so-called mixed HMMs. This class of models has several interesting features. It deals with the dependence of a response variable on covariates, serial dependence, and unobserved heterogeneity in an HMM framework. It exploits the properties of HMMs, such as the relatively simple dependence structure and the efficient computational procedure, and allows one to handle a variety of real-world time-dependent data. We give details of the Expectation-Maximization algorithm for computing the maximum likelihood estimates of model parameters and we illustrate the method with two real applications describing the relationship between patent counts and research and development expenditures, and between stock and market returns via the Capital Asset Pricing Model.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>2</TD><TD width='5%' bordercolor='blue'>Dans cet article, nous passons en revue les méthodes statistiques combinant les modèles de Markov cachés (HMMs) et les modèles longitudinaux à effets aléatoires. Cette combinaison constitue la classe des modèles dits de Markov cachés mixtes, ou mixtures de modèles de Markov cachés. Cette classe présente plusieurs avantages. Il modélise la dépendance d'une réponse en des covariables, la dépendance sérielle, et l'hétérogénéité non observée. Il exploite les propriétés des HMMs, telles leur structure de dépendance relativement simple, et les procédés de calcul efficaces qui y sont attachés; il permet en outre la prise en compte d'une variété de grandeurs dépendant du temps. Nous étudions dans le détail l'algorithme "Expectation-Maximization" pour l'estimation par maximum de vraisemblance des paramètres du modèle, et illustrons la méthode au moyen de deux applications à des données réelles: relations entre nombre de brevets déposés et dépenses de recherche/développement, et relations entre rendements d'actions et rendement du marché dans le cadre du modèle CAPM.</TD></TR></table><P align='left' style='color:#993300'><B><u>10.2307/41305060</u></B></P><BR><P align='left' style='color:#CC66FF'><B><u>Current Issuemap</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>The paper estimates a large-scale mixed-frequency dynamic factor model for the euro area, using monthly series along with gross domestic product (GDP) and its main components, obtained from the quarterly national accounts (NA). The latter define broad measures of real economic activity (such as GDP and its decomposition by expenditure type and by branch of activity) that we are willing to include in the factor model, in order to improve its coverage of the economy and thus the representativeness of the factors. The main problem with their inclusion is not one of model consistency, but rather of data availability and timeliness, as the NA series are quarterly and are available with a large publication lag. Our model is a traditional dynamic factor model formulated at the monthly frequency in terms of the stationary representation of the variables, which however becomes nonlinear when the observational constraints are taken into account. These are of two kinds: nonlinear temporal aggregation constraints, due to the fact that the model is formulated in terms of the unobserved monthly logarithmic changes, but we observe only the sum of the monthly levels within a quarter, and nonlinear cross-sectional constraints, since GDP and its main components are linked by the NA identities, but the series are expressed in chained volumes. The paper provides an exact treatment of the observational constraints and proposes iterative algorithms for estimating the parameters of the factor model and for signal extraction, thereby producing nowcasts of monthly GDP and its main components, as well as measures of their reliability.</TD></TR></table><P align='left' style='color:#FF9900'><B><u>Most Eligible Issuemap Info</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>The paper estimates a large-scale mixed-frequency dynamic factor model for the euro area, using monthly series along with gross domestic product (GDP) and its main components, obtained from the quarterly national accounts (NA). The latter define broad measures of real economic activity (such as GDP and its decomposition by expenditure type and by branch of activity) that we are willing to include in the factor model, in order to improve its coverage of the economy and thus the representativeness of the factors. The main problem with their inclusion is not one of model consistency, but rather of data availability and timeliness, as the NA series are quarterly and are available with a large publication lag. Our model is a traditional dynamic factor model formulated at the monthly frequency in terms of the stationary representation of the variables, which however becomes nonlinear when the observational constraints are taken into account. These are of two kinds: nonlinear temporal aggregation constraints, due to the fact that the model is formulated in terms of the unobserved monthly logarithmic changes, but we observe only the sum of the monthly levels within a quarter, and nonlinear cross-sectional constraints, since GDP and its main components are linked by the NA identities, but the series are expressed in chained volumes. The paper provides an exact treatment of the observational constraints and proposes iterative algorithms for estimating the parameters of the factor model and for signal extraction, thereby producing nowcasts of monthly GDP and its main components, as well as measures of their reliability.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>2</TD><TD width='5%' bordercolor='blue'>Cet article présente l'estimation d'un modèle à facteurs dynamiques multifréquences de grande dimension pour l'activité économique de la zone euro. La base de données analysée comprend diverses séries mensuelles, ainsi que les évaluations trimestrielles des Produits Nationaux Bruts (PNB) et de leurs principales composantes, telles qu'elles apparaissent dans les publications trimestrielles des Comptes Nationaux. Ces dernières constituent des mesures de l'activité économique réelle (ainsi, la ventilation du PNB par branche d'activité) que nous désirons introduire dans le modèle à facteurs de façon à accroître la représentativité des facteurs. Le problème est que les données relatives aux PNB sont trimestrielles, et publiées avec un délai plus ou moins long. Notre modèle est un modèle à facteurs traditionnel, formulé aux fréquences mensuelles en termes de représentation stationnaire des variables. Cette formulation devient non linéaire, toutefois, quand les contraintes liées à l'observation sont prises en compte. Ces contraintes sont de deux types: contraintes observationnelles liées à l'agrégation temporelle non linéaire (le modèle fait intervenir des variations logarithmiques mensuelles non observables, alors que seules sont observées leurs sommes trimestrielles), et contraintes longitudinales non linéaires liées à la nature des données de type PNB. Nous fournissons un traitement exact des contraintes observationnelles, et proposons des algorithmes itératifs pour l'estimation du modèle à facteurs. Cette estimation permet le "nowcasting" des PNB mensuels et de leurs composantes, ainsi qu'une mesure de la fiabilité des "nowcasts" ainsi obtenus.</TD></TR></table><P align='left' style='color:#993300'><B><u>10.2307/41305058</u></B></P><BR><P align='left' style='color:#CC66FF'><B><u>Current Issuemap</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>We review probabilistic and graphical rules for detecting situations in which a dependence of one variable on another is altered by adjusting for a third variable (i. e., non-collapsibility or noninvariance under adjustment), whether that dependence is causal or purely predictive. We focus on distinguishing situations in which adjustment will reduce, increase, or leave unchanged the degree of bias in an association that is taken to represent a causal effect of one variable on the other. We then consider situations in which adjustment may partially remove or introduce a bias in estimating causal effects, and some additional special cases useful for case-control studies, cohort studies with loss, and trials with non-compliance (non-adherence).</TD></TR></table><P align='left' style='color:#FF9900'><B><u>Most Eligible Issuemap Info</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>We review probabilistic and graphical rules for detecting situations in which a dependence of one variable on another is altered by adjusting for a third variable (i. e., non-collapsibility or noninvariance under adjustment), whether that dependence is causal or purely predictive. We focus on distinguishing situations in which adjustment will reduce, increase, or leave unchanged the degree of bias in an association that is taken to represent a causal effect of one variable on the other. We then consider situations in which adjustment may partially remove or introduce a bias in estimating causal effects, and some additional special cases useful for case-control studies, cohort studies with loss, and trials with non-compliance (non-adherence).</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>2</TD><TD width='5%' bordercolor='blue'>Nous passons en revue les méthodes probabilistes et graphiques dans la détection de situations où la dépendance d'une première variable par rapport à une seconde variable se trouve modifiée par la prise en compte d'une troisième variable, que cette dépendance soit de nature causale ou purement prédictive. Nous mettons l'accent, en particulier, sur la détection des cas où la prise en compte d'une tierce variable entraîne la réduction ou l'augmentation du biais des mesures d'association représentant l'impact causal d'une variable sur l'autre. Nous considérons ensuite les situations dans lesquelles la tierce variable est susceptible d'annuler, ou de biaiser l'estimation des effets de causalité, ainsi que quelques cas particuliers utiles dans les études de cas-témoins, les études de cohortes, et les essais en présence de non-observance (non-compliance).</TD></TR></table><P align='left' style='color:#993300'><B><u>10.2307/41305054</u></B></P><BR><P align='left' style='color:#CC66FF'><B><u>Current Issuemap</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>This paper presents the development of the African Statistical Development Index, a composite index that aims at supporting the monitoring and evaluation of the implementation of the Reference Regional Strategic Framework for Statistical Capacity Building in Africa. It also helps to identify, for each African country, weaknesses and strengths of the National Statistical Systems so that support interventions can be developed. The paper first gives the rationale behind the development of the index as well as the context. It then elaborates on the methodology used to develop the index, including the selection of components and variables, the scaling of the variables, the weighting and aggregation schemes, and the validation process. The methodology is applied to a sample of African countries. Finally, the paper compares the proposed index to existing statistical capacity building indicators and highlights the related limitations.</TD></TR></table><P align='left' style='color:#FF9900'><B><u>Most Eligible Issuemap Info</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>This paper presents the development of the African Statistical Development Index, a composite index that aims at supporting the monitoring and evaluation of the implementation of the Reference Regional Strategic Framework for Statistical Capacity Building in Africa. It also helps to identify, for each African country, weaknesses and strengths of the National Statistical Systems so that support interventions can be developed. The paper first gives the rationale behind the development of the index as well as the context. It then elaborates on the methodology used to develop the index, including the selection of components and variables, the scaling of the variables, the weighting and aggregation schemes, and the validation process. The methodology is applied to a sample of African countries. Finally, the paper compares the proposed index to existing statistical capacity building indicators and highlights the related limitations.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>2</TD><TD width='5%' bordercolor='blue'>Cet article présente l'Indice de développement statistique africain, un indice composite ayant pour objectif de supporter le suivi et l'évaluation de la mise en oeuvre du Cadre stratégique régional de référence pour le renforcement des capacités statistiques en Afrique. Cet indice permet, entre autres, d'identifier les forces et faiblesses du système statistique national de chaque pays africain en vue de favoriser des interventions ciblées de la part des intervenants. Pour ce faire, cet article présente la raison d'être et le contexte entourant le développement de l'indice. Par la suite, il s'attarde sur la méthodologie entourant son développement incluant la sélection des composantes et des variables, les pondérations et le processus d'agrégation ainsi que celui de validation. Il présente une application de la méthodologie sur un échantillon des pays africains et compare finalement l'indice à d'autres indices de développement statistique existants sans oublier les limitations relatives à son utilisation.</TD></TR></table><P align='left' style='color:#993300'><B><u>10.2307/41305055</u></B></P><BR><P align='left' style='color:#CC66FF'><B><u>Current Issuemap</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>We study parametric and non-parametric approaches for assessing the accuracy and coverage of a population census based on dual system surveys. The two parametric approaches being considered are post-stratification and logistic regression, which have been or will be implemented for the US Census dual system surveys. We show that the parametric model-based approaches are generally biased unless the model is correctly specified. We then study a local post-stratification approach based on a non-parametric kernel estimate of the Census enumeration functions. We illustrate that the non-parametric approach avoids the risk of model mis-specification and is consistent under relatively weak conditions. The performances of these estimators are evaluated numerically via simulation studies and an empirical analysis based on the 2000 US Census post-enumeration survey data.</TD></TR></table><P align='left' style='color:#FF9900'><B><u>Most Eligible Issuemap Info</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>We study parametric and non-parametric approaches for assessing the accuracy and coverage of a population census based on dual system surveys. The two parametric approaches being considered are post-stratification and logistic regression, which have been or will be implemented for the US Census dual system surveys. We show that the parametric model-based approaches are generally biased unless the model is correctly specified. We then study a local post-stratification approach based on a non-parametric kernel estimate of the Census enumeration functions. We illustrate that the non-parametric approach avoids the risk of model mis-specification and is consistent under relatively weak conditions. The performances of these estimators are evaluated numerically via simulation studies and an empirical analysis based on the 2000 US Census post-enumeration survey data.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>2</TD><TD width='5%' bordercolor='blue'>Nous étudions des méthodes paramétriques et non paramétriques d'évaluation de la précision et de la couverture d'un recensement fondé sur un double système d'enquêtes. Deux approches paramétriques, la post-stratification et la régression logistique, sont considérées; ces approches ont été ou seront mises en pratique dans le cadre du double système d'enquêtes utilisé pour le recensement des Etats-Unis. Nous montrons que ces méthodes sont généralement biaisées lorsque le modèle sur lequel elles se fondent est incorrectement spécifié. Nous étudions ensuite une approche de post-stratification locale fondée sur un estimateur à noyau des fonctions d'enumeration du recensement. Nous illustrons le fait que cette approche non paramétrique évite les risques liés aux erreurs de spécification des modèles utilisés, et converge sous des conditions assez générales. Les performances des estimateurs qui en résultent sont évaluées de façon numérique à partir de simulations et d'une analyse empirique fondée sur les résultats de l'enquête post-censitaire du recensement des Etats-Unis de 2000.</TD></TR></table><P align='left' style='color:#993300'><B><u>10.2307/41305056</u></B></P><BR><P align='left' style='color:#CC66FF'><B><u>Current Issuemap</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>In most countries, national statistical agencies do not release establishment-level business microdata, because doing so represents too large a risk to establishments' confidentiality. One approach with the potential for overcoming these risks is to release synthetic data; that is, the released establishment data are simulated from statistical models designed to mimic the distributions of the underlying real microdata. In this article, we describe an application of this strategy to create a public use file for the Longitudinal Business Database, an annual economic census of establishments in the United States comprising more than 20 million records dating back to 1976. The U. S. Bureau of the Census and the Internal Revenue Service recently approved the release of these synthetic microdata for public use, making the synthetic Longitudinal Business Database the first-ever business microdata set publicly released in the United States. We describe how we created the synthetic data, evaluated analytical validity, and assessed disclosure risk.</TD></TR></table><P align='left' style='color:#FF9900'><B><u>Most Eligible Issuemap Info</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>In most countries, national statistical agencies do not release establishment-level business microdata, because doing so represents too large a risk to establishments' confidentiality. One approach with the potential for overcoming these risks is to release synthetic data; that is, the released establishment data are simulated from statistical models designed to mimic the distributions of the underlying real microdata. In this article, we describe an application of this strategy to create a public use file for the Longitudinal Business Database, an annual economic census of establishments in the United States comprising more than 20 million records dating back to 1976. The U. S. Bureau of the Census and the Internal Revenue Service recently approved the release of these synthetic microdata for public use, making the synthetic Longitudinal Business Database the first-ever business microdata set publicly released in the United States. We describe how we created the synthetic data, evaluated analytical validity, and assessed disclosure risk.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>2</TD><TD width='5%' bordercolor='blue'>Dans la plupart des pays, les instituts nationaux de statistique ne publient pas les micro-données relatives aux entreprises. Les publier présente en effet un risque trop élevé de rupture de confidentialité. Ce risque peut être évité par un recours à des données synthétiques— des données simulées à partir de modèles statistiques reproduisant la loi des véritables micro-données. Dans cet article, nous décrivons une application de cette stratégie à la création d'une telle base de données à partir des résultats du recensement économique annuel des entreprises américaines. Cette base de donnée comprend plus de 20 millions d'entreprises sur une période remontant à 1976. L'U.S. Bureau of Census et l'Internai Revenue Service ont récemment approuvé la publication sous forme synthétique de ces micro-données, faisant ainsi de la Longitudinal Business Database le premier ensemble de micro-données de ce type accessible au public aux Etats-Unis. Nous expliquons la façon dont cette base de données synthétiques a été créée, comment sa validité a été testée, et comment son risque de rupture de confidentialité a été évalué.</TD></TR></table><P align='left' style='color:#993300'><B><u>10.2307/41305057</u></B></P><BR><P align='left' style='color:#CC66FF'><B><u>Current Issuemap</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>We present a modern perspective of the conditional likelihood approach to the analysis of capturerecapture experiments, which shows the conditional likelihood to be a member of generalized linear model (GLM). Hence, there is the potential to apply the full range of GLM methodologies. To put this method in context, we first review some approaches to capture-recapture experiments with heterogeneous capture probabilities in closed populations, covering parametric and non-parametric mixture models and the use of covariates. We then review in more detail the analysis of capturerecapture experiments when the capture probabilities depend on a covariate.</TD></TR></table><P align='left' style='color:#FF9900'><B><u>Most Eligible Issuemap Info</u></B></P><BR><table width='100%'><TR valign='top'><TD width='5%' bordercolor='blue'>1</TD><TD width='5%' bordercolor='blue'>We present a modern perspective of the conditional likelihood approach to the analysis of capturerecapture experiments, which shows the conditional likelihood to be a member of generalized linear model (GLM). Hence, there is the potential to apply the full range of GLM methodologies. To put this method in context, we first review some approaches to capture-recapture experiments with heterogeneous capture probabilities in closed populations, covering parametric and non-parametric mixture models and the use of covariates. We then review in more detail the analysis of capturerecapture experiments when the capture probabilities depend on a covariate.</TD></TR><TR valign='top'><TD width='5%' bordercolor='blue'>2</TD><TD width='5%' bordercolor='blue'>Nous présentons une perspective moderne de l'approche par vraisemblances conditionnelles de l'analyse des expériences de capture-recapture. Nous montrons que ces vraisemblances conditionnelles relèvent d'un modèle linéaire généralisé, ce qui permet l'application des nombreuses méthodes élaborées dans ce cadre. Pour replacer ces applications dans leur contexte, nous passons en revue quelques-unes des approches existantes dans les modèles de capture-recapture avec probabilités de capture hétérogènes au sein de populations fermées. Nous décrivons, en particulier, l'utilisation de modèles de mélange paramétriques et non paramétriques, et examinons de façon plus détaillée le cas où les probabilités de capture sont fonction de covariables.</TD></TR></table>
				
		</td>
		</tr>
	</table>		
</body>
</html>
